{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Lab 2: Time Series Cross Validation\n",
    "\n",
    "**In this computer lab practical you will learn**\n",
    "\n",
    "* How to apply time series cross validation to select the best forecasting model.\n",
    "\n",
    "> To keep things simple we will work with naive forecasting models.\n",
    "\n",
    "---\n",
    "* A video introducing the notebook and a walk through of exercise 1 is available https://bit.ly/ts_cross_val\n",
    "* A youtube video to help with python's generators (optional) https://www.youtube.com/watch?v=2eiFCQ-YAf4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast-tools imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if running in Google Colab install forecast-tools\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip install forecast-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from forecast_tools.baseline import SNaive, Naive1\n",
    "from forecast_tools.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Implementing rolling forecast origin time series cross validation.\n",
    "\n",
    "# Exercise 1.1: Load the data\n",
    "\n",
    "**Task**:\n",
    "\n",
    "* Import monthly outpatient appointments time series. This can be found in **\"out_appoints_mth.csv\"**\n",
    "\n",
    "**Hints:** \n",
    "* This is monthly data.  You can use the Monthly Start ('MS') frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here ...\n",
    "url = 'https://raw.githubusercontent.com/health-data-science-OR/' \\\n",
    "        + 'hpdm097-datasets/master/out_appoints_mth.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.2. Create a rolling forecast origin in a loop.\n",
    "\n",
    "In rolling forecast origin we iteratively move our model into the future.  Assume we have the following training data.\n",
    "\n",
    "```python\n",
    "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "``` \n",
    "\n",
    "If we initially train our model on the first 5 data points and make a 1 step forecast, then a rolling forecast origin look like this:\n",
    "\n",
    "```python\n",
    "[1] train => [1, 2, 3, 4, 5], test => [6]\n",
    "[2] train => [1, 2, 3, 4, 5, 6], test => [7]\n",
    "[3] train => [1, 2, 3, 4, 5, 6, 7], test => [8]\n",
    "[4] train => [1, 2, 3, 4, 5, 6, 7, 8], test => [9]\n",
    "[5] train => [1, 2, 3, 4, 5, 6, 7, 8, 9], test => [10]\n",
    "``` \n",
    "\n",
    "If instead we had a forecast horizon of 2 then our splits would look like:\n",
    "```python\n",
    "[1] train => [1, 2, 3, 4, 5], test => [6, 7]\n",
    "[2] train => [1, 2, 3, 4, 5, 6], test => [7, 8]\n",
    "[3] train => [1, 2, 3, 4, 5, 6, 7], test => [8, 9]\n",
    "[4] train => [1, 2, 3, 4, 5, 6, 7, 8], test => [9, 10]\n",
    "``` \n",
    "\n",
    "There are different ways to implement this in Python.  The simplest is to use a for loop.\n",
    "\n",
    "**Task:**\n",
    "    \n",
    "* Code a rolling forecast origin for-loop that returns a `train` and `test` in each iteration of a training data input.\n",
    "* In each cv-fold the length of test should be equal to a parameter `h` that represents the forecast horizon\n",
    "* In each iteration the length of the training set should increase by 1. \n",
    "* You will need to work out how many iterations you should make!  Be careful not to go over the end of the dataset!\n",
    "* You will need to select a initial training set size.  This could be for example a third of the data.\n",
    "\n",
    "**Hints:**\n",
    "* Try working with the following data set initially.  Make sure it gives you the results you expect.\n",
    "\n",
    "```python\n",
    "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "``` \n",
    "\n",
    "* The data above is held in a python `list`, in practice you will be passing in a `pandas.Series` or a `np.ndarray` all of these are **array-like** and accept python slice notation for example:\n",
    "\n",
    "```python\n",
    "train_fold = train[:5]\n",
    "```\n",
    "\n",
    "* In the above you could vary `5` using variables in a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.3: Calculate the average MAE of SNaive\n",
    "\n",
    "Now that you have the basic structure of rolling forecast origin returning train-test splits you can use it to iteratively fit a model and calculate its forecast accuracy.  We will work with `SNaive` and the outpatients dataset.\n",
    "\n",
    "**Task**:\n",
    "\n",
    "* Modify your code from 1.2. to train and predict using a `Naive1` model \n",
    "* In each iteration calculate and store the mean absolute error (MAE) \n",
    "* At the end of the loop calculate the mean cross validation score.\n",
    "\n",
    "**Hints**:\n",
    "* Declare a python list called `scores` and append the MAE of each split during the loop.\n",
    "* This is monthly data so Seasonal Naive requires a parameter of 12.\n",
    "* Don't forget to set the size of the initial training data.  E.g. half or a third of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.4: Refactoring your code as a generator\n",
    "\n",
    "No matter which forecasting approach you use, you will do a lot of cross validation in practice.  To help, it is good to aim to get all of your code into a reusable format and start builing your own utility functions. For cross-validation there are a number of ways to do it.  One option is to convert your for-loop into a **generator**. \n",
    "\n",
    "**Task:**\n",
    "* Refactor your code from 1.2 into a generator.  \n",
    "\n",
    "**Hints:**\n",
    "* Generators make use of the **yield** statement. At the end of each iteration you need to yield a tuple that contains (train, test).  \n",
    "* Assuming you call your generator `rolling_forecast_origin`, The code that would use your generator could look like:\n",
    "\n",
    "```python\n",
    "train = np.arange(24)\n",
    "model = SNaive(12)\n",
    "cv_rolling = rolling_forecast_origin(train, min_train_size=5, horizon=2)\n",
    "scores = []\n",
    "\n",
    "for train_fold, test_fold in cv_rolling:\n",
    "    model.fit(train)\n",
    "    preds = model.predict(horizon)\n",
    "    score = mean_absolute_error(test_fold, preds)\n",
    "    scores.append(score)\n",
    "     \n",
    "print(np.array(scores).mean())\n",
    "```\n",
    "\n",
    "* The advantage of splittng the implementation of logic that does the splitting and the forecasting is that you can reuse `rolling_forecast_origin` with any forecasting method regardless of its interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Choosing the best naive model\n",
    "\n",
    "We will work with ED reattendance dataset from the previous computer lab.  Your task is to select the best naive model.  Last time we used a simple holdout sample, this time we will use cross validation.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "* Load the ED reattendance dataset 'ed_reattends.csv' (url provide below)\n",
    "* Use a forecast horizon of 6 months with an initial training size of 1/3 of the dataset.\n",
    "* Test all Naive models: `Naive1`, `SNaive`, `Average`, `Drift`, `EnsembleNaive`\n",
    "* Select the forecast with the lowest mean absolute error.\n",
    "\n",
    "**Hints:**\n",
    "* This is monthly data - remember to seasonaly adjust \n",
    "* forecast-tools provides a convenience function to load all naive models\n",
    "\n",
    "```python\n",
    ">>> from forecast_tools.baseline import baseline_estimators\n",
    ">>> estimators = baseline_estimators(seasonal_period=12)\n",
    ">>> estimators\n",
    "\n",
    "{'NF1': Naive1(),\n",
    " 'SNaive': SNaive1(period=12),\n",
    " 'Average': Average(),\n",
    " 'Drift': Drift(),\n",
    " 'Ensemble': EnsembleNaive(seasonal_period=12)}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here ...\n",
    "url = 'https://raw.githubusercontent.com/health-data-science-OR/' \\\n",
    "        + 'hpdm097-datasets/master/ed_reattend.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: using forecast-tools built in cross-validation options\n",
    "\n",
    "`forecast-tools` provides some optimised tools to help you with cross validation in the `forecast_tools.model_selection` module.\n",
    "\n",
    "```python\n",
    ">>> from forecast_tools.model_selection import (rolling_forecast_origin, \n",
    "                                                sliding_window)\n",
    "```\n",
    "\n",
    "These functions are implementations of Rolling Forecast Origin and Sliding Window splitting of the time series.  For example, for a rolling forecast origin split\n",
    "\n",
    "```python\n",
    ">>> min_train_size = len(reattend_rate) // 3\n",
    ">>> cv = rolling_forecast_origin(train=reattend_rate, \n",
    "                                 min_train_size=min_train_size, \n",
    "                                 horizon=6, \n",
    "                                 step=1)\n",
    "```\n",
    "\n",
    "Notice that `rolling_forecast_origin` has an additional parameter `step`. If this is set to greater than 1 then this introduces a gap between folds.  This might used for instance if the time series is very large or if there is a desire to seperate the folds to reduce their similarity.\n",
    "\n",
    "`forecast-tools` also contains utility functions to either return a cv scores or cv predictions.\n",
    "\n",
    "```python\n",
    "\n",
    ">>> from forecast_tools.model_selection import (cross_validation_score, \n",
    "                                                cross_validation_folds)\n",
    "```\n",
    "\n",
    "The first of these functions returns a numpy array of the forecast errors for each fold and the latter returns a numpy array the train, test and y_preds in each fold.\n",
    "\n",
    "```python\n",
    ">>> from forecast_tools.metrics import root_mean_squared_error\n",
    ">>> nf1 = Naive1()\n",
    ">>> scores = cross_validation_score(model=nf1, cv=cv, \n",
    "                                    metric=root_mean_squared_error,\n",
    "                                    n_jobs=-1)\n",
    ">>> print(scores.mean())\n",
    "\n",
    "4.074747242185809\n",
    "```\n",
    "\n",
    "`cross_validation_score` accepts any metric with a `metric(y_true, y_pred)` pattern. So you could use `mean_absolute_error` or another of the metrics in place of `root_mean_squared_error`.  The parameters `n_jobs=-1` means that the cross validation is conducted in parallel.  You can also assess multiple forecast horizons in one go by using the horizons parameter.  For example if you wanted to report the cv scores at 1-6 month intervals set `horizons=[1, 2, 3, 4, 5, 6]`\n",
    "\n",
    "```python\n",
    ">>> nf1 = Naive1()\n",
    ">>> horizons = [1, 2, 3, 4, 5, 6]\n",
    ">>> scores = cross_validation_score(model=nf1, cv=cv, \n",
    "                                    metric=root_mean_squared_error, \n",
    "                                    horizons=horizons,\n",
    "                                    n_jobs=-1)\n",
    "\n",
    ">>> print(pd.DataFrame(scores, columns=horizons).mean())\n",
    "\n",
    "1    3.052419\n",
    "2    3.510499\n",
    "3    3.605947\n",
    "4    3.759665\n",
    "5    3.944258\n",
    "6    4.074747\n",
    "dtype: float64\n",
    "```\n",
    "\n",
    "# Exercise 3.1 Repeat exercise 2 using forecast-tools.\n",
    "\n",
    "**Task**:\n",
    "* Load the ED reattendance dataset 'ed_reattends.csv'\n",
    "* Use a forecast horizon of 6 months with an initial training size of 1/3 of the dataset.\n",
    "* Test all Naive models: `Naive1`, `SNaive`, `Average`, `Drift`, `EnsembleNaive`\n",
    "* Select the forecast with the lowest mean absolute error.\n",
    "* Use forecast-tools to do the cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2: cross validation of multiple forecast horizons\n",
    "\n",
    "**Task:** \n",
    "    \n",
    "* Using forecast-tools and the ed_reattend.csv data how do Naive1 and EnsembleNaive compare over forecast horizons of 1 to 6 months?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.3: automatically selecting the best naive method\n",
    "\n",
    "`forecast-tools` provides an `auto_naive()` function to automatically select the 'best' naive method to serve as a benchmark.  This works via cross-validation.  The tool has a number of settings, but for practical purposes most of these can be left to the defaults.  \n",
    "\n",
    "```python\n",
    ">>> from forecast_tools.model_selection import auto_naive\n",
    ">>> benchmark = auto_naive(y_train=reattend_rate, horizon=6, seasonal_period=12)\n",
    ">>> print(benchmark)\n",
    "\n",
    "{'model': EnsembleNaive(seasonal_period=12), 'mae': 3.091713319664855}\n",
    "```\n",
    "\n",
    "The function returns a dict that contains both the best model and the metric of interest (by default the MAE).\n",
    "\n",
    "**Task:**\n",
    "* Use `auto_naive` to select a benchmark model for the outpatient appointments dataset 'out_appoints_mth.csv'\n",
    "* Try running the function using a different metric such as MAPE.  Use `help(auto_naive)` to see how parameters work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
