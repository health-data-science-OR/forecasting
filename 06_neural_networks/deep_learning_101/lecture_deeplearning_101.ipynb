{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning 101\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "This lecture takes a practical approach to introduce modern deep learning approaches.  It provides foundational deep learning knowledge in order for you to move onto time series forecasting.\n",
    "</div>\n",
    "\n",
    "**By the end of this lecture you will have:**\n",
    "    \n",
    "* Developed a conceptual understanding of modern deep neural networks.\n",
    "* Built intuition about what hidden layers within a deep network are doing and how they aid prediction.\n",
    "* Understand the definition and benefits of mini batches of training data\n",
    "* Built intuition about how networks 'learn' using the backpropogation algorithm and stochastic gradient descent.\n",
    "* Learnt how to build simple neural network architectures in Keras and Tensorflow 2.0\n",
    "* The foundational knowledge to move onto using feedforward neural networks for time series forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras and Tensorflow Imports\n",
    "\n",
    "For your deep learning you will make use of [Keras](https://keras.io/).  This is a python library that sits on top of Google's deep learning toolset: Tensorflow 2.0.  Keras makes deep learning relatively straightfoward because it hides a lot of the complexity of Tensorflow. \n",
    "\n",
    "> Another very powerful deep learning framework is [PyTorch](https://pytorch.org/).  This is a pythonic deep learning toolkit and is also very powerful.  Our research experience is that PyTorch is more efficient than Keras and Tensorflow (sometimes by a considerable margin), but that it requires more code to do the same things as Keras.  Another way to look at this is that Keras comes with 'more bells and whistles' than PyTorch and for learning that comes in very handy!  The exercises that you will tackle in this course are written in Keras/TF, but you will also have access to optional material written in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# if using hds_forecast this should be version 2.7\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational cost of deep learning\n",
    "\n",
    "When you have a complex deep learning architecture (which isn't always the case) and lots of data you should expect it to be more computationally expensive (take longer to run and work your CPU hard) than other types of ML.  In these instances, you really need a powerful machine and for some models a GPU.  For **time series forecasting**, we will be using the OpenStack on the High Performance Cluster, but for personal learning and coursework you can also make use of Google Colaboratory (Jupyter in the cloud).  Google also provide a GPU.  All of the neural network notebooks in this course are runnable in Google Colab. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A first look at Deep Learning using Tensorflow Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Tensorflow playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.55467&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false) is provided by Google.  I recommend that you spend some time using it as it helps build intuition about how deep learning works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components needed for deep learning.\n",
    "\n",
    "## 1. Training and test data\n",
    "\n",
    "The data for this lecture is a real dataset known as the [Wisconsin Breast Cancer](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)) dataset. The data set consists of individual   we will use a **Feedforward Neural Network Architecture.**\n",
    "\n",
    "The data are published and open; feel free to take a look at them in more detail.\n",
    "\n",
    "> The dataset contains 30 features and a binary label (benign/malignant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/MichaelAllen1966/' \\\n",
    "       + 'synthetic_data_pilot/main/01_wisconsin/wisconsin.csv'\n",
    "cancer = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and drop 'id' column\n",
    "cancer.drop('id', axis=1, inplace=True)\n",
    "\n",
    "# Change 'diagnosis' column to 'malignant', and put in last column place\n",
    "cancer['malignant'] = cancer['diagnosis'] == 'M'\n",
    "cancer.drop('diagnosis', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30 features and a single binary (0/1) label\n",
    "cancer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split\n",
    "\n",
    "Just like other Machine Learning approaches we first do a train test split (and do not look at or use our test data until we are ready!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into x and y for split\n",
    "X = cancer[cancer.columns[:-1]]\n",
    "y = cancer['malignant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting random_state means we always get the same split\n",
    "X_train, X_test, y_train, y_test \\\n",
    "    = train_test_split(X.to_numpy(), y.to_numpy(), \n",
    "                       test_size=0.20, \n",
    "                       random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class balance\n",
    "unique_elements, counts_elements = np.unique(y_train, return_counts=True)\n",
    "print(unique_elements, counts_elements)\n",
    "print(y_train.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rescale data\n",
    "\n",
    "You should always rescale the features that you use to train a neural network.\n",
    "\n",
    "I recommend scaling **after** a train-test split where the scaler uses the **training** data.  In a production setting you cannot scale on the data you are about to predict!  You should aim to simulate this setting and avoid leakage!\n",
    "\n",
    "Here we will use [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) to rescale features to range from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will rescale all data to be between 0 and 1\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the rescaling and assign to dataframe\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# scale training data (cast to dataframe)\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(X_train))\n",
    "X_train_scaled.columns = cancer.columns[:-1]\n",
    "\n",
    "# scale test data (cast to dataframe)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test))\n",
    "X_test_scaled.columns = cancer.columns[:-1]\n",
    "\n",
    "# take a look at the rescaled data\n",
    "print(X_train_scaled.shape)\n",
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential layers and activation functions\n",
    "\n",
    "Feedforward neural networks accept a vector of quantitative values as input and pass this through a sequence of fully connected layers (all neurons are connected to each other).  Each neuron in a layer recieves input from all of the neurons in the previous layer. The neuron weights the input vector, adds bias and then passes it through an activation function. In a hidden layer, for example, you could use a Rectified Linear Unit (ReLU; $f(x) = \\max\\left(0, x\\right)$).  \n",
    "\n",
    "The final layer is an **output** layer.  The thrombolysis example is a binary classification so the output layer is a fully connected layer with a single neuron.  You will need to use a `sigmoid` activation function to provide a probability of recieving thromboysis between 0.0 and 1.0.\n",
    "\n",
    "For feedforward networks, `Keras` provides simple classes to help you construct your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a model consisting of a sequential set of layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# a fully connected layer, an input layer\n",
    "from tensorflow.keras.layers import Dense, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first input\n",
    "model = Sequential(name='breast_cancer_nn')\n",
    "\n",
    "# input layer shape = (no. features, )\n",
    "model.add(Input(shape=(X_train.shape[1],)))\n",
    "\n",
    "# hidden layer 1: relu: f(x) = max(0, x)\n",
    "model.add(Dense(units=10, activation='relu'))\n",
    "\n",
    "# hidden layer 2\n",
    "model.add(Dense(units=10, activation='relu'))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# summary including number of trainable parameters\n",
    "model.summary()\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a network\n",
    "\n",
    "For each neuron you can think of the weights as the **strength** of the neuron's **connections** to all of the neurons in the **previous layer**.  The network is initialised with these weights set to **random** values.  The purpose of training is therefore to find the 'best' weights for your prediction problem.   \n",
    "\n",
    "In this context, 'best' means the weights that minimise the training **loss** (sometimes called **cost** or **error**).  Loss is a measure of model fit i.e. a metric quantifying the difference between the predictions from the model and the ground truth observations. For classification models, you will use **Binary Cross-Entropy**.  For each training example, this measure simply takes the -log of the probability the model assigned to the correct label and then averages across all samples.  Or more formally, $$-\\dfrac{1}{N} \\sum_{i=1}^N y_i \\cdot log(p(y_i)) + (1 - y_i) \\cdot log(1 - p(y_i))$$ \n",
    "\n",
    "where $y$ is the 0/1 label and $p(y_i)$ is the probability (0 to 1) assigned to predicting label $i$.  \n",
    "\n",
    "> Note that in a regression model, you would use a loss metric such as **Mean Absolute Error** or **Mean Squared Error**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's assume these are the probs assigned to a TRUE value\n",
    "cost = [0.1, 0.2, 0.8, 0.99]\n",
    "-np.log(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent and Backpropagation\n",
    "\n",
    "Think of finding the best weights for your model as a large scale optimisation problem. Even in the simple first model we created there were 400+ parameters to optimise! To optimise these weights an algorithm is needed to estimate the gradient of the loss function and taking a **step** to gradually **decend** it into a local optima. \n",
    "\n",
    "<img src=\"gradient1.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "Finding these local optima is achieved using the **backpropagation algorithm** and **stochastic gradient descent**.  After pushing a training sample through our binary classification network we get a single probability of the patient having cancer. We also have the ground truth value labelling if the patient has a malignant tumor or not. Starting from the networks output layer, backprop works backward through each layer of the network to find the weights and biases that correctly classify the patient. This is repeated for all of the training data and the average of these values is the gradient of the loss function with respect to each weight. Repeated enough times the networks weights will converge on solution that minimises the loss. \n",
    "\n",
    "<img src=\"iteration.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "For large networks and datasets this is computationally expensive (and possibly infeasible) to run backpropagation against every training sample individually. Therefore we use **stochastic gradient decent (SGD)** to estimate the gradient.  The gradient is estimated by averaging results after breaking the dataset into random **mini-batches** (subsets). This reduces number of computations needed by a substantial amount (and makes large problems tractible).  \n",
    "\n",
    "<img src=\"yanlcun.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "The size of the step taken at each iteration is called the **learning rate**. It is used to subtract a fraction of the gradient from the current weights. The learning rate is a hyperparameter and may need tuning.  A typical starting point is to try powers of 10; for example, 0.1, 0.01, 0.001.  A larger step means that you can descend more quickly!  The downside is that you might overshoot the minimum!  \n",
    "\n",
    "In practice, SGD is not quite so simple and there are a few other concepts and optimisations procedures to appreciate.  The first is that it useful to tweak SGD to have **momentum**. The modification adds a fraction (typically 0.9) of the previous update to weights. The second is adapative learning rates. This comes in different flavours from a simple reduction in learning rate over time to varying the learning rate by parameter.  Both momentum and adaptive learning rates help SGD converge more efficiently.\n",
    "\n",
    "<img src=\"overshoot.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "\n",
    "\n",
    "### Training in Keras\n",
    "\n",
    "In Keras, the complexity described above is hidden and you simply **compile** a model and choose an `optimizer` and `loss` function.  You then call the model's `fit` method and pass in the training data, mini batch size (32 is the default) and the number of **epochs** to run.  An epoch is a single pass through all of the training data.  If, for example, there are 1000 points and you are using a mini-batchs of 100 then there are 10 iterations within a single epoch.  You need to run multiple epochs for the network to descend the loss function.\n",
    "\n",
    "I recommend you choose the Adam optimiser.  This is the most popular version of stochastic gradient descent since about 2015.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the optimizer, loss function and also report classification accuracy\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# fit the model and include a validation split to check for overfitting. \n",
    "results = model.fit(x=X_train_scaled.to_numpy(), \n",
    "                    y=y_train, \n",
    "                    batch_size=32,\n",
    "                    validation_split=0.10, \n",
    "                    epochs=200, \n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(results):\n",
    "    '''\n",
    "    Two charts 1.) train versus validation loss and 2.) accuracy\n",
    "    '''\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(12,6))\n",
    "    ax[0].plot(results.history['loss'])\n",
    "    ax[0].plot(results.history['val_loss'])\n",
    "    ax[0].legend(['loss', 'val_loss'])\n",
    "    ax[1].plot(results.history['accuracy'])\n",
    "    ax[1].plot(results.history['val_accuracy'])\n",
    "    ax[1].legend(['accurracy', 'val accurracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting\n",
    "\n",
    "Given enough capacity, neural networks will overfit to your training data.  Models that are overfitted have highly variable performance when used for prediction.  Keras provides a couple of simple mechanisms to reduce overfitting: **dropout layers** and **early stopping callbacks**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_size, n_hidden=2, n_neurons=10, activation='relu', \n",
    "              dropout=False, d_rate=0.2):\n",
    "    '''\n",
    "    Create a simple Keras feedforward model.\n",
    "    '''\n",
    "    #The first input\n",
    "    model = Sequential(name='breast_cancer_nn')\n",
    "\n",
    "    #input layer.\n",
    "    model.add(Input(shape=(input_size,)))\n",
    "\n",
    "    for i in range(n_hidden):\n",
    "        #hidden layer 1 \n",
    "        model.add(Dense(units=n_neurons, activation=activation))\n",
    "        #include a dropout layer\n",
    "        if dropout:\n",
    "            model.add(Dropout(d_rate))\n",
    "    \n",
    "    #output layer\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ General Parameters ############################\n",
    "\n",
    "N_HIDDEN = 3\n",
    "N_NEURONS = 32\n",
    "N_EPOCHS = 200\n",
    "\n",
    "# 0 fit silently; 1 show results per epoch\n",
    "VERBOSE = 0\n",
    "\n",
    "########### Regularization options ########################\n",
    "INCLUDE_DROPOUT = True\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "INCLUDE_EARLY_STOP = True\n",
    "PATIENCE = 10\n",
    "\n",
    "#create an early stopping callback\n",
    "es = EarlyStopping(monitor='val_loss', \n",
    "                   patience=PATIENCE,\n",
    "                   restore_best_weights=True)\n",
    "\n",
    "###########################################################\n",
    "\n",
    "#include early stopping?\n",
    "callbacks = []\n",
    "if INCLUDE_EARLY_STOP:\n",
    "    callbacks.append(es)\n",
    "\n",
    "#get the custom feedforward model\n",
    "model_2 = get_model(input_size=X_train.shape[1], \n",
    "                    n_hidden=N_HIDDEN,\n",
    "                    n_neurons=N_NEURONS,\n",
    "                    dropout=INCLUDE_DROPOUT,\n",
    "                    d_rate=DROPOUT_RATE)\n",
    "\n",
    "#summary to remind us what we have built!\n",
    "print(model_2.summary())\n",
    "\n",
    "#compile the new model\n",
    "model_2.compile(optimizer='adam',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#fit the model and also pass in the callback\n",
    "results_2 = model_2.fit(x=X_train_scaled, \n",
    "                        y=y_train, \n",
    "                        batch_size=32,\n",
    "                        validation_split=0.10, \n",
    "                        epochs=N_EPOCHS, \n",
    "                        verbose=VERBOSE,\n",
    "                        callbacks=callbacks)\n",
    "\n",
    "#plot loss and val loss\n",
    "plot_loss(results_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "Predicting the training set very straightforward making use of the models `.predict()` method.  \n",
    "\n",
    "> When predicting results for individual patients you will need to `.reshape` your input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled.to_numpy()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled.to_numpy()[0].reshape(1, -1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting and individual patients result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = 1\n",
    "\n",
    "pred = model_2.predict(x=X_test_scaled.to_numpy()[test_id].reshape(1, -1))[0,0]\n",
    "print(f'prediction proba {pred:.2f}')\n",
    "print(f'prediction: {pred >= 0.5}')\n",
    "print(f'ground truth value: {y_test[test_id]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.predict(x=X_test_scaled.to_numpy()[test_id].reshape(1, -1))[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the full test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_2.predict(x=X_test_scaled.to_numpy()).flatten()\n",
    "np.round(y_pred, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quick reminder of classification metrics\n",
    "\n",
    "TP = True positives\n",
    "FP = False positives\n",
    "TN = True negatives\n",
    "FN = False negatives\n",
    "\n",
    "$precision = \\dfrac{TP}{TP + FP}$  e.g if model predicts a patient does have a maligant tumor with precision 0.8.  Then when a model makes a positive cancer prediction it is right about 80% of the time.\n",
    "\n",
    "$recall = sensitivity = \\dfrac{TP}{TP + FN}$ I.e. The proportion of true positive identified. \n",
    "\n",
    "$specificity =  \\dfrac{TN}{TN + FP}$  How many negative classifications are actually negative?\n",
    "\n",
    "For the Winconsin breast cancer dataset we want a high sensitivity (detecting as many patients tumors as possible), but not at the cost of specificity (lots of false positives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predictions\n",
    "y_pred = model_2.predict(x=X_test_scaled.to_numpy()).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions are probabilities that the patient has a malignant tumour\n",
    "np.round(y_pred, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view as 0/1\n",
    "THRESHOLD = 0.5\n",
    "(y_pred >= THRESHOLD).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification results\n",
    "THRESHOLD = 0.5\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred >= THRESHOLD).flatten()\n",
    "\n",
    "#sensitivity and specificity\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(f'sensitivity {sensitivity:.3f}')\n",
    "print(f'specificity {specificity:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_test, y_pred >= THRESHOLD, digits=3)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Uncertainty\n",
    "\n",
    "One issue with neural networks is that they don't automatically produce estimates of uncertainty.  This is problematic as they have been trained using stochastic gradient descent. \n",
    "\n",
    "One way to get an estimate of uncertainty from a neural network is to use **Monte Carlo Dropout**.  We have already learnt about `Dropout` layers and using them for regularisation.  What perhaps isn't clear is that when we make a prediction with a Keras model the `Dropout` layers are turned **off**.  If we instead turn them **on** and making a large number of predictions from the same data produce a distribution of predictions.  \n",
    "\n",
    "> The below will only work **if you include dropout layers in your model!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predictions\n",
    "sample_no = 77\n",
    "y_pred = model_2.predict(x=X_test_scaled.to_numpy()[sample_no].reshape(1, -1)).flatten()\n",
    "print(np.round(y_pred, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[sample_no]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probas = [model_2(X_test_scaled.to_numpy()[sample_no].reshape(1, -1), training=True) \n",
    "            for sample in range(100)]\n",
    "\n",
    "y_probas = np.round(np.stack(y_probas).flatten(), 2)\n",
    "y_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.quantile(y_probas, q=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probas.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probas.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
