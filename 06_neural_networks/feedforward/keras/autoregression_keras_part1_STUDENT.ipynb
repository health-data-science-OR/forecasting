{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoregressive model using a feedforward neural network\n",
    "\n",
    "In this notebook we will use a feedforward neural network to fit a linear model to time series data. \n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "1. Unlike purpose built time series models such as ARIMA we need to preprocess the data beforehand.\n",
    "\n",
    "2. Forecasting **h-steps** ahead is more involved than other methods and can use either an iterative, direct (multiple-model) method or vector output methodology.\n",
    "</div>\n",
    "\n",
    "---\n",
    "**LEARNING OBJECTIVES**\n",
    "\n",
    "* Preprocess time series data to a format compatabile with a feedforward neural network\n",
    "* Generate h-step forecasts using \n",
    "    * an iterative approach\n",
    "    * a direct modelling approach\n",
    "    * a vector output layer approach\n",
    "* Construct a deep feedforward neural network for forecasting\n",
    "---\n",
    "\n",
    "> **If you are struggling with the neural network aspects of this notebook then please note there is a [Ordinary Least Squares regression version you can explore](../../../05_autoregression/autoregression1.ipynb).  This covers the preprocessing and iterative and direct methods for forecasting.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Python dependencies\n",
    "\n",
    "It is recommended that you use the forecasting course conda environment provided for this work `hds_stoch`. We are going to implement neural networks using `tensorflow` and '`keras`. You should be using **at least** `tensorflow` version `2.1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "#tensorflow/keras imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "#turn of tensorflow warnings when using predict in a loop.\n",
    "#tf.get_logger().setLevel('WARNING')\n",
    "\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The forecasting process for Feedforward neural networks\n",
    "\n",
    "1. Select $l$ the number of autoregressive lags and forecast horizon $h$\n",
    "2. Preprocess the data into tabular form [[$lag_1, lag_2, ... lag_l$], [$y_t$]]\n",
    "3. Train the NN model using the tabular data\n",
    "4. Iteratively forecast 1-step ahead gradually replacing ground truth observations with predictions.\n",
    "\n",
    "\n",
    "### 2.1 Synthetic data without noise\n",
    "\n",
    "Given the extra complexities of forecasting using OLS we will use simple synthetic data before exploring real healthcare data. The synthetic data we wil use is a cosine.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(200)\n",
    "ts_data = np.cos(0.2 * t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ts_data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Preprocess the time series into tabular autoregressive form\n",
    "\n",
    "An autoregressive model consists of $l$ lags of the time series. \n",
    "\n",
    "An easy way to think about the form of the data for a autoregressive OLS model is as a table of variables.  The first $l$ columns are the lags (the independent predictor variables) and the final column is $y$ at time $t$ ($y_t$) that is, the target/dependent variable.  \n",
    "\n",
    "We there need to manipulate the time series so that is now in that format.  More precisely for each row we need: \n",
    "\n",
    "**A vector presenting the lags at time t**\n",
    "* $X_t = $ [$lag_{t-l}, ... lag_{t-2}, lag_{t-1}$]\n",
    "\n",
    "**A scalar value representing y at time t:**\n",
    "* $y_t$\n",
    "\n",
    "For training we need a vector of rows ($X_t$) and vector of target $y_t$. e.g.\n",
    "\n",
    "```python\n",
    "X_train = [X_1, X_2, X_3, ...,  X_t]\n",
    "\n",
    "y_train = [y_1, y_2, y_3, ..., y_t]\n",
    "```\n",
    "---\n",
    "\n",
    "The function `sliding_window` illustrates how to preprocess time series data into tabular form  in python.  Below the function is an example script to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(train, window_size=2, horizon=1):\n",
    "    '''\n",
    "    sliding window.\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    train: array-like\n",
    "        training data for time series method\n",
    "    \n",
    "    window_size: int, optional (default=2)\n",
    "        lookback - how much lagged data to include.\n",
    "        \n",
    "    horizon: int, optional (default=1)\n",
    "        number of observations ahead to predict\n",
    "            \n",
    "    Returns:\n",
    "        array-like, array-like\n",
    "    \n",
    "        preprocessed X, preprocessed Y\n",
    "    '''\n",
    "    tabular_X = []\n",
    "    tabular_y = []\n",
    "    \n",
    "    for i in range(0, len(train) - window_size - horizon):\n",
    "        X_train = train[i:window_size+i]\n",
    "        y_train = train[i+window_size+horizon-1]\n",
    "        tabular_X.append(X_train)\n",
    "        tabular_y.append(y_train)\n",
    "       \n",
    "    return np.asarray(tabular_X), np.asarray(tabular_y).reshape(-1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess time series training and test sets\n",
    "X_train, y_train = sliding_window(ts_data, window_size=2)\n",
    "\n",
    "training_length = int(len(y_train) * (2/3))\n",
    "\n",
    "X_train, X_test = X_train[:training_length], X_train[training_length:]\n",
    "y_train, y_test = y_train[:training_length], y_train[training_length:]\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'ts_data[:3]: {ts_data[:3]}')\n",
    "print(f'X_train[0]: {X_train[0]}')\n",
    "print(f'y_train[0]: {y_train[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'ts_data[1:4]: {ts_data[1:4]}')\n",
    "print(f'X_train[1]: {X_train[1]}')\n",
    "print(f'y_train[1]: {y_train[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Just to be absolutely clear what we are doing let's convert the numpy arrays to a **pandas.DataFrame**.  In this format you should be able to see why the a regression model (such an OLS or a feedforward neural network can be used for autoregression in a time series.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_form = pd.concat([pd.DataFrame(X_train), pd.DataFrame(y_train)], axis=1)\n",
    "columns = [f'lag_{i}' for i in range(len(X_train[0]), 0, -1)]\n",
    "columns.append('y_t')\n",
    "tabular_form.columns = columns\n",
    "tabular_form.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Train the NN using the preprocessed series\n",
    "\n",
    "After preprocessing the data, fitting the data is relatively straightforward.  We create and compile an instance of a `Keras.models.Sequential` pass in the training data and call the `.fit()` method.  The method fit returns results that can be used to check for overfitting and convergence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_model(ws, lr=0.01, metrics=None):\n",
    "    '''\n",
    "    OLS model implemented as a ff neural netweork\n",
    "    '''\n",
    "    if metrics is None:\n",
    "        metrics = ['mae', 'mse']\n",
    "    \n",
    "    model = Sequential([Dense(1, input_shape=(ws,))])\n",
    "    model.compile(loss='mse', \n",
    "                  optimizer=Adam(lr=lr),\n",
    "                  metrics=metrics)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#script to train the model\n",
    "\n",
    "#set tensorflow random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "#### Try changing these parameters ###############\n",
    "N_EPOCHS = 100\n",
    "WINDOW_SIZE = 12\n",
    "es = EarlyStopping(monitor='val_loss', patience=10)\n",
    "##################################################\n",
    "\n",
    "#pre-process the data into sliding windows\n",
    "X_train, y_train = sliding_window(ts_data, window_size=WINDOW_SIZE)\n",
    "\n",
    "#train test split\n",
    "training_length = int(len(y_train) * (2/3))\n",
    "X_train, X_test = X_train[:training_length], X_train[training_length:]\n",
    "y_train, y_test = y_train[:training_length], y_train[training_length:]\n",
    "\n",
    "#compile the tf model\n",
    "model = get_linear_model(WINDOW_SIZE)\n",
    "\n",
    "results = model.fit(x=X_train, \n",
    "                    y=y_train, \n",
    "                    epochs=N_EPOCHS,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    verbose=0,\n",
    "                    callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot training and validation loss\n",
    "plt.plot(results.history['loss'], label='loss')\n",
    "plt.plot(results.history['val_loss'], label='val_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Forecasting 1 step ahead\n",
    "\n",
    "To forecast 1-step ahead we use treat the network as we would for other regression problems i.e. we call the `.predict()` method.  For example if we wanted to forecast the first y observation in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x=X_test[0].reshape(1, -1))[0,0]\n",
    "print(f'1-step forecast: {pred}')\n",
    "print(f'ground trust value: {y_test[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Forecast h periods ahead using the iterative method.\n",
    "\n",
    "**We have trained our `NN` model to predict 1-step**. When forecasting 2 or more steps ahead we still only have five ground truth observations ($lag_1$ ... $lag_5$). This means that when forecasting h-steps ahead we need to do this in a loop where we iteratively replace our ground truth observations with our predictions.\n",
    "\n",
    "Let's assume we are forecasting 3 periods ahead and have a model with 5 lags.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = [100, 120, 110, 95, 135]\n",
    "print(f'Input for forecasting t+1 = {X_1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_1 = 111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2 = X_1[1:] + [forecast_1] \n",
    "print(f'Input for forecasting t+2 = {X_2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_2 = 222"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_3 = X_2[1:] + [forecast_2] \n",
    "print(f'Input for forecasting t+3 = {X_3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's an easy way implemented iterative forecasting in python using the `np.roll(a, shift)` function.  That shifts everything in the array down by `shift`.  The function is **circular** so the value in element 0 is moved to be the final value in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_3 = 999\n",
    "\n",
    "X_4 = np.roll(X_3, shift=-1)\n",
    "print(f'after np.roll {X_4}')\n",
    "X_4[-1] = forecast_3\n",
    "print(f'after inserting last forecast: {X_4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logic above is implemented in the reusable function `autoregressive_iterative_forecast`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoregressive_iterative_forecast(model, exog, h):\n",
    "    '''\n",
    "    h-step forecast for an autoregressive \n",
    "    model using the iterative prediction method.\n",
    "    \n",
    "    Conduct h one-step forecasts gradually\n",
    "    replacing ground truth autoregressive X \n",
    "    values with predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    ------\n",
    "    model: forecast object\n",
    "        model that has a .predict(h) interface\n",
    "        \n",
    "    exog: array-like\n",
    "        initial vector of lagged values (X)\n",
    "    \n",
    "    h: int\n",
    "        forecast horizon. assumed to be > 0\n",
    "    \n",
    "    Returns:\n",
    "    ------\n",
    "    numpy.ndarray\n",
    "        y_predictions\n",
    "    '''\n",
    "    y_preds = []\n",
    "    current_X = exog\n",
    "    for i in range(h):\n",
    "        #1 step forecast\n",
    "        y_pred = model.predict(current_X.reshape(1, -1))[0,0]\n",
    "        #store prediction\n",
    "        y_preds.append(y_pred)\n",
    "        \n",
    "        #roll the array and insert forecast\n",
    "        current_X = np.roll(current_X, shift=-1)\n",
    "        current_X[-1] = y_pred\n",
    "\n",
    "    return np.array(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the function\n",
    "H = 5\n",
    "y_preds = autoregressive_iterative_forecast(model, X_test[0], h=H)\n",
    "print(f'Iterative forecast: {y_preds}')\n",
    "print(f'Ground truth y: {y_test[:H].T}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding some noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this a bit more interesting we will add some normally distributed noise to the synthetic time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the random seed so that we all get the same results\n",
    "np.random.seed(12)\n",
    "t = np.arange(200)\n",
    "ts_data = np.cos(0.2 * t)\n",
    "noise = np.random.normal(loc=0.0, scale=0.2, size=200)\n",
    "ts_data = ts_data + noise\n",
    "plt.plot(ts_data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set tensorflow random seed\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "N_EPOCHS = 100\n",
    "WINDOW_SIZE = 12\n",
    "es = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "#preprocess time series training and test sets\n",
    "X_train, y_train = sliding_window(ts_data, window_size=WINDOW_SIZE)\n",
    "\n",
    "training_length = 130\n",
    "\n",
    "#train-test split\n",
    "X_train, X_test = X_train[:training_length], X_train[training_length:]\n",
    "y_train, y_test = y_train[:training_length], y_train[training_length:]\n",
    "\n",
    "#compile the tf model\n",
    "model = get_linear_model(WINDOW_SIZE, metrics=['mse'], lr=0.001)\n",
    "\n",
    "#fit model silently\n",
    "results = model.fit(x=X_train, \n",
    "                    y=y_train, \n",
    "                    epochs=N_EPOCHS,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    verbose=0,\n",
    "                    callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot results\n",
    "plt.plot(results.history['loss'], label='loss')\n",
    "plt.plot(results.history['val_loss'], label='val_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the fitted values against the ground truth observations.  To get the fitted values you need to call `.predict()` and pass in `X_train` (the X training data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ts_data[WINDOW_SIZE+1:], label='ground truth')\n",
    "plt.plot(model.predict(X_train), label='NN fitted')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally you make the forecast you need to call `autoregressive_iterative_forecast`.  \n",
    "\n",
    "> Note that you only need to pass in the first element from `X_test` i.e. `X_test[0]`.  This is because you are making an **iterative** forecast that starts from the latest ground truth data and works it way for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make iterative predictions\n",
    "H = len(y_test)\n",
    "y_preds_iter = autoregressive_iterative_forecast(model, X_test[0], h=H)\n",
    "\n",
    "#plot against test data\n",
    "plt.plot(y_preds_iter, label='iterative forecast method')\n",
    "plt.plot(y_test, label='ground truth')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.5 The direct h-step forecasting method.\n",
    "\n",
    "In the direct method to forecast h-steps ahead we have **$h$ forecasting models**.  Each model provides a single point forecast from a step ahead.  In the example here, y_test is of length 57 periods.  The direct method requires 57 NNs to make its prediction!\n",
    "\n",
    "\n",
    "Recall the `sliding_window` function.  We ignored an optional parameter `horizon` in the iterative example.  By default `horizon=1` i.e. the function returns target values that are only a single period ahead.  We can vary the step size by increasing the value of horizon.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training multiple models**\n",
    "\n",
    "1. Create a for loop and set it to iterate 57 times. \n",
    "2. In each loop call `sliding_window` setting `horizon` to the iteration number + 1\n",
    "3. Create a new instance of the the model\n",
    "4. Train the model and save in a list.\n",
    "5. Save the model to .h5 file. (recommended so you can reload without retraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_direct_models(ts, n_epochs, horizon, window_size, \n",
    "                        train_length, save=False):\n",
    "    '''\n",
    "    Train multiple models.  One for each forecast horizon.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    ts: array-like\n",
    "        The FULL time series\n",
    "        \n",
    "    n_epochs: int\n",
    "        Number of epoch for training\n",
    "        \n",
    "    horizon: int\n",
    "        Forecast horizon in time units of the series\n",
    "        \n",
    "    window_size: int\n",
    "        The number of lags to include in each model\n",
    "        \n",
    "    train_length: int\n",
    "        Used in train test splot for each model\n",
    "        \n",
    "    save: bool, optional (default=False)\n",
    "        Save the trained models to file.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "        list of models\n",
    "        \n",
    "    '''\n",
    "    models = []\n",
    "\n",
    "    print('Training model =>', end=' ')\n",
    "    for h in range(horizon):\n",
    "        print(f'{h+1}', end=', ')\n",
    "        #preprocess time series training and test sets\n",
    "        X_train, y_train = sliding_window(ts, \n",
    "                                          window_size=window_size, \n",
    "                                          horizon=h+1)\n",
    "\n",
    "        #train-test split\n",
    "        X_train, X_test = X_train[:train_length], X_train[train_length:]\n",
    "        y_train, y_test = y_train[:train_length], y_train[train_length:]\n",
    "\n",
    "        #compile the tf model\n",
    "        model_h = get_linear_model(window_size, metrics=['mse'], lr=0.001)\n",
    "\n",
    "    \n",
    "        #fit model silently (verbose=0)\n",
    "        results_h = model_h.fit(x=X_train, \n",
    "                            y=y_train, \n",
    "                            epochs=n_epochs,\n",
    "                            verbose=0)\n",
    "        \n",
    "        if save:\n",
    "            model_h.save(f'./output/direct_model_h{h+1}.h5')\n",
    "\n",
    "        models.append(model_h)\n",
    "\n",
    "    print('done')\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(horizon):\n",
    "    '''\n",
    "    load pre-trained models.\n",
    "    '''\n",
    "    models = []\n",
    "    for h in range(HORIZON):\n",
    "        model_h = tf.keras.models.load_model(f'output/direct_model_h{h+1}.h5')\n",
    "        models.append(model_h)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set tensorflow random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "N_EPOCHS = 100\n",
    "HORIZON = len(y_test)\n",
    "WINDOW_SIZE = 12\n",
    "TRAIN_LENGTH = 130\n",
    "LOAD_FROM_FILE = False\n",
    "\n",
    "if LOAD_FROM_FILE:\n",
    "    direct_models = load_models(HORIZON)\n",
    "else:\n",
    "    direct_models = train_direct_models(ts_data, \n",
    "                                        n_epochs=N_EPOCHS,\n",
    "                                        horizon=HORIZON, \n",
    "                                        window_size=WINDOW_SIZE, \n",
    "                                        train_length=TRAIN_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the `direct_forecast` function.  This is just a for loop to call the `.predict()` method of each model.  Remember that the input to each model is **same** i.e. exog which in our case will be `X_test[0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_forecast(models, exog):\n",
    "    '''\n",
    "    h-step forecast for an autoregressive \n",
    "    model using the direct prediction method.\n",
    "    \n",
    "    Each model contained in @models has been trained\n",
    "    to predict a unique number of steps ahead. \n",
    "    Each model provides a single forecast and the results are \n",
    "    combined in an ordered array and returned.\n",
    "    \n",
    "    Parameters:\n",
    "    ------\n",
    "    models: list\n",
    "        direct models each has has a .predict(exog) \n",
    "        interface\n",
    "        \n",
    "    exog: array-like\n",
    "        initial vector of lagged values (X)\n",
    "        \n",
    "    Returns:\n",
    "    ------\n",
    "    numpy.ndarray\n",
    "        y_predictions\n",
    "    '''\n",
    "    preds = []\n",
    "    for model_h in models:\n",
    "        pred_h = model_h.predict(x=exog.reshape(1, -1))[0, 0]\n",
    "        preds.append(pred_h)\n",
    "    \n",
    "    return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the direct forecast\n",
    "y_preds_direct = direct_forecast(direct_models, X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for clarity repeat the preprocessing\n",
    "X_train, y_train = sliding_window(ts_data, window_size=WINDOW_SIZE)\n",
    "\n",
    "#train-test split\n",
    "X_train, X_test = X_train[:TRAIN_LENGTH], X_train[TRAIN_LENGTH:]\n",
    "y_train, y_test = y_train[:TRAIN_LENGTH], y_train[TRAIN_LENGTH:]\n",
    "\n",
    "#plot the direct forecast against the test data\n",
    "plt.plot(y_preds_direct, label='direct forecast method')\n",
    "plt.plot(y_test, label='ground truth')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the iterative method the direct method looks a close match to the ground truth test set!  Let's plot all three datasets on the same chart and then take a look at the **RMSE** of each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot iterative and direct\n",
    "plt.plot(y_preds_direct, label='direct forecast method')\n",
    "plt.plot(y_preds_iter, label='iterative forecast method')\n",
    "plt.plot(y_test, label='ground truth', linestyle='-.')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tools.eval_measures import rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rmse iterative method\n",
    "rmse(y_test, y_preds_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rmse direct method\n",
    "rmse(y_test, y_preds_direct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular example (and single holdout set) the iterative method out performed the direct method. You should not assume this is always the case!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises: Forecasting a vector of y\n",
    "\n",
    "In the **iterative** and **direct** methods we always forecast a *scalar* value.  An modification is to adapt a feedforward neural network to predict a **vector of y values**.  Using this architecture we would train our model on sliding windows of $X$ and $y$.  Where y is a vector of length $h$ and $X$ is a vector of length $ws$ (window size)\n",
    "\n",
    "## Exercise 1: preprocessing the time series into vectors of y\n",
    "**Task:**\n",
    "* modify the function `sliding_window` (provided below) so that it returns a vectors of y.\n",
    "\n",
    "Hints:\n",
    "* Assume you are standing at time $t$. With a forecasting horizon of $h$, y would be $[y_{t+1}, y_{t+2}, ... , y_{t+h}]$.\n",
    "* Array slicing might prove useful:\n",
    "\n",
    "```python\n",
    "train = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "print(train[2:6])\n",
    "```\n",
    "```\n",
    ">> [3 4 5 6]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to modify\n",
    "def sliding_window(train, window_size=2, horizon=1):\n",
    "    '''\n",
    "    sliding window.\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    train: array-like\n",
    "        training data for time series method\n",
    "    \n",
    "    window_size: int, optional (default=2)\n",
    "        lookback - how much lagged data to include.\n",
    "        \n",
    "    horizon: int, optional (default=1)\n",
    "        number of observations ahead to predict\n",
    "            \n",
    "    Returns:\n",
    "        array-like, array-like\n",
    "    \n",
    "        preprocessed X, preprocessed Y\n",
    "    '''\n",
    "    tabular_X = []\n",
    "    tabular_y = []\n",
    "    \n",
    "    for i in range(0, len(train) - window_size - horizon):\n",
    "        ########### MODIFICATIONS NEEDED HERE ###########################\n",
    "        X_train = train[i:window_size+i]\n",
    "        y_train = train[i+window_size+horizon-1]\n",
    "        ################################################################\n",
    "        tabular_X.append(X_train)\n",
    "        tabular_y.append(y_train)\n",
    "       \n",
    "    return np.asarray(tabular_X), np.asarray(tabular_y).reshape(-1, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After** you have modified `sliding_window` run the code below to preprocess the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 12\n",
    "HORIZON = 12\n",
    "TRAIN_LENGTH = 130\n",
    "#for clarity repeat the preprocessing\n",
    "X_train, y_train = sliding_window(ts_data, \n",
    "                                  window_size=WINDOW_SIZE,\n",
    "                                  horizon=HORIZON)\n",
    "\n",
    "#train-test split\n",
    "X_train, X_test = X_train[:TRAIN_LENGTH], X_train[TRAIN_LENGTH:]\n",
    "y_train, y_test = y_train[:TRAIN_LENGTH], y_train[TRAIN_LENGTH:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Build setup a model that predicts vectors in Keras\n",
    "\n",
    "**Task**\n",
    "* Create a function called `get_target_vector_network_model` that creates and returns a `Sequential` model with **one** hidden (`Dense`) layer and a output layer that predicts 12 time periods ahead.\n",
    "* Use relu activation in your hidden layers.\n",
    "* Use the adam opimizer with a learning rate of 0.01 (but feel free to try others).\n",
    "* Train the model.\n",
    "\n",
    "**Hints**\n",
    "* Remember an output layer is just a `Dense` layer with 12 neurons.\n",
    "* You may want to set a tensorflow random seed so that you can repeat your results. (e.g. 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Predict a single vector ahead.\n",
    "\n",
    "Predicting a single vector ahead is actually making a h-step forecast.  This is done in exactly the same way as the other models using `.predict(X)`. For example assuming your model is called `model_v` you would run the following code to make a prediction.\n",
    "\n",
    "```python\n",
    "#to understand why we include [0] on the end comment it out and rerun the code.\n",
    "y_preds = model_v.predict(X_test[0].reshape(1, -1))[0]\n",
    "```\n",
    "\n",
    "**Task**\n",
    "* Use your trained model to predict 12 time points (a single vector length) ahead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: predicting multiple vectors ahead (> h-steps)\n",
    "\n",
    "It is important to remember that with the vector output you predict in multiples of $h$.  So if you make two predictions you have predictions for 2h.  But in general this works in the same way as the iterative method.  Each time you forecast you replace $h$ values in the X vector with the predictions.\n",
    "\n",
    "**Task**: \n",
    "* Use the function `vector_iterative_forecast` (provided below) with your new vector outpuit model. \n",
    "* Predict 4 vector lengths ahead and plot the result against the test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_iterative_forecast(model, exog, h):\n",
    "    '''\n",
    "    h-step forecast for an autoregressive \n",
    "    model using the iterative prediction method.\n",
    "    \n",
    "    Conduct h one-step forecasts gradually\n",
    "    replacing ground truth autoregressive X \n",
    "    values with predictions.\n",
    "    \n",
    "    Assumes that h is less than or equal to the length of \n",
    "    exog\n",
    "    \n",
    "    Parameters:\n",
    "    ------\n",
    "    model: forecast object\n",
    "        model that has a .predict(h) interface\n",
    "        \n",
    "    exog: array-like\n",
    "        initial vector of lagged values (X)\n",
    "    \n",
    "    h: int\n",
    "        forecast horizon. assumed to be > 0\n",
    "    \n",
    "    Returns:\n",
    "    ------\n",
    "    numpy.ndarray\n",
    "        y_predictions\n",
    "    '''\n",
    "    y_preds = []\n",
    "    current_X = exog\n",
    "    for i in range(h):\n",
    "        y_pred = model.predict(current_X.reshape(1, -1))[0]\n",
    "        y_preds.append(y_pred)\n",
    "        \n",
    "        #update the input array for the iterative prediction\n",
    "        v_len = len(y_pred)\n",
    "        current_X = np.roll(current_X, shift=-v_len)\n",
    "        current_X[-v_len:] = y_pred.copy()\n",
    "        \n",
    "    return np.concatenate(np.array(y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Lab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
