{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoregressive models using a feedforward neural network \n",
    "## PART 2: Applying the methods to health care time series\n",
    "\n",
    "In this notebook we will use a feedforward neural network to fit a single and ensemble linear and non-linear models to real time series data. \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "1. Most of the work we will do is data manipulation: preprocessing data and making sure it is the right shape for the neural networks.\n",
    "\n",
    "2. The ensemble learning method can be computationally expensive.  We have included some pre-trained models that can be loaded from file if your machine is not powerful or you cannot use Google Collabratory.\n",
    "</div>\n",
    "\n",
    "---\n",
    "**LEARNING OBJECTIVES**\n",
    "\n",
    "* Learn how to apply feedforward neural networks to real health data.\n",
    "* Methods to preprocess nn input data.\n",
    "* Recognise the stochastic nature of neural network training\n",
    "* Use a ensemble of neural networks to provide a more reliable point forecast\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Python dependencies\n",
    "\n",
    "It is recommended that you use the forecasting course conda environment provided. We are again going to implement neural networks using `tensorflow` and '`keras`. You should be using at least `tensorflow` version `2.1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "#tensorflow imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Forecasting emergency admissions in England\n",
    "\n",
    "We will now use feedforward neural networks to predict the number of monthly emergency admissions in England. \n",
    "\n",
    "### 2.1 Load the data\n",
    "\n",
    "**Task**:\n",
    "* Execute the code below to read the emergency admissions data into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/health-data-science-OR/data/master/em_admits_ts.csv'\n",
    "em_admits = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_admits.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_admits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Preprocessing \n",
    "\n",
    "### 2.3.1 Datetime format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the the `month_year` column in `em_admits` holds a string an invalid date format e.g. 'Aug-10'.  Pandas cannot handle this as-is because '10' could refer to any century!  So let's do a bit of preprocessing to get it into a valid datetime format.\n",
    "\n",
    "\n",
    "*Optional Task:*\n",
    "* Take some time to understand the code that preprocesses the dates.  This is real health data and it is likely you will need to deal with formatting issues as experienced here.\n",
    "\n",
    "First we will format the string to something pandas can parse i.e. 'Aug 2010'.  Then we will call the `pd.to_datetime()` function to parse the string and return a `datetime`.  We will assign the result to our dataframe's index and set the freq to monthly start 'MS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = em_admits['month_year'].str[:3] + ' 20' + em_admits['month_year'].str[-2:]\n",
    "date_str.name = 'date'\n",
    "em_admits = em_admits.set_index(pd.to_datetime(date_str))\n",
    "em_admits.index.freq = 'MS'\n",
    "em_admits = em_admits.drop(columns=['month_year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_admits.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Visualise the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be forecasting the last 12 months of the series.  Let's take a look at the training data (being careful to exclude the last 12 months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_length = 12\n",
    "em_admits[:len(em_admits)-holdout_length].plot(figsize=(12,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Calender adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is monthly data so a useful preprocessing step is to transform the data into a daily rate by dividing by the number of days in the month. When we plot this the troughs we saw in Feb each year disappear.\n",
    "\n",
    "**Exercise**:\n",
    "* Calculate the average admissions per day series\n",
    "* Plot the training data (holding back 12 months for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admit_rate = em_admits['em_admits'] / em_admits.index.days_in_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admit_rate[:len(admit_rate)-12].plot(figsize=(12,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Convert the time series to format suitable for supervised learning.\n",
    "\n",
    "Exercise:\n",
    "* Using a sliding window approach convert the time series into a tabular format. \n",
    " * Use a window size of 12 and assume you are predicting a scalar value of y (1-step ahead).\n",
    "* Conduct a train test split holding back 12 windows for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(train, window_size=2, horizon=1):\n",
    "    '''\n",
    "    sliding window.\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    train: array-like\n",
    "        training data for time series method\n",
    "    \n",
    "    window_size: int, optional (default=2)\n",
    "        lookback - how much lagged data to include.\n",
    "        \n",
    "    horizon: int, optional (default=1)\n",
    "        number of observations ahead to predict\n",
    "            \n",
    "    Returns:\n",
    "        array-like, array-like\n",
    "    \n",
    "        preprocessed X, preprocessed Y\n",
    "    '''\n",
    "    tabular_X = []\n",
    "    tabular_y = []\n",
    "    \n",
    "    for i in range(0, len(train) - window_size - horizon):\n",
    "        X_train = train[i:window_size+i]\n",
    "        y_train = train[i+window_size+horizon-1]\n",
    "        tabular_X.append(X_train)\n",
    "        tabular_y.append(y_train)\n",
    "       \n",
    "    return np.asarray(tabular_X), np.asarray(tabular_y).reshape(-1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_train_test_split(*arrays, train_size):\n",
    "    '''\n",
    "    time series train test split\n",
    "    \n",
    "    Parameters:\n",
    "    X: array-like\n",
    "        X data\n",
    "    y_data \n",
    "    '''\n",
    "    results = ()\n",
    "    for a in arrays:\n",
    "        results += a[:train_size], a[train_size:]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 12\n",
    "\n",
    "X_data, y_data = sliding_window(admit_rate, \n",
    "                                window_size=WINDOW_SIZE)\n",
    "\n",
    "#train test split\n",
    "train_size = len(y_data) - 12\n",
    "X_train, X_test, y_train, y_test = ts_train_test_split(X_data, \n",
    "                                                       y_data,\n",
    "                                                       train_size=train_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 Scaling the features and target to be between -1 and 1\n",
    "In many machine learning applications data are scaled to be between 0 and 1. For neural network forecasting, *Ord, Fildes and Kourentzes (2017)* recommend scaling to be between -1 and 1.  This is what we will do here.  To do the scaling we will use\n",
    "\n",
    "```python\n",
    "sklearn.preprocessing.MinMaxScaler\n",
    "```\n",
    "\n",
    "* Execute the code below to transform the data.\n",
    "\n",
    "**TO ADD: short video or seperate notebook of how min max scaler works**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "#I am scaling on admit_rate because this will include the first 12 lags \n",
    "#not in y_train\n",
    "scaler.fit(admit_rate.iloc[:-12].to_numpy().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = scaler.transform(y_train.reshape(-1, 1))\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "y_test = scaler.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 A Linear regression model benchmark\n",
    "\n",
    "The first model we will try is the linear model. Its will serve as our neural network baseline.  (In practice we would also check this is better than a naive method such as seasonal naive).\n",
    "\n",
    "### 2.4.1. Train the model\n",
    "\n",
    "**Exercise:**\n",
    "* Using `Keras`, construct a neural network that mimics a simple linear regression model (see previous notebook).  \n",
    "* Optional: To get comparable results, set the tensorflow random number seed to 1234\n",
    "* Train the model for 100 epochs.\n",
    "* Optionally you can use an early stopping callback with patience set to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_model(ws, lr=0.01, metrics=None):\n",
    "    if metrics is None:\n",
    "        metrics = ['mae', 'mse']\n",
    "    \n",
    "    model = Sequential([Dense(1, input_shape=(ws,))])\n",
    "    model.compile(loss='mse', \n",
    "                  optimizer=Adam(lr=lr),\n",
    "                  metrics=metrics)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set tensorflow random seed for repeatability\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "N_EPOCHS = 100\n",
    "es = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "#call the linear model function create earlier.\n",
    "model_lm = get_linear_model(ws=12, metrics=['mae'])\n",
    "\n",
    "#fit model silently (verbose=0)\n",
    "history = model_lm.fit(x=X_train, \n",
    "                    y=y_train, \n",
    "                    epochs=N_EPOCHS,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    verbose=0,\n",
    "                    callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Plot the fitted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scaler.inverse_transform(y_train), label='ground truth')\n",
    "plt.plot(scaler.inverse_transform(model_lm.predict(X_train)), label='NN fitted')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 Generate and evaluate a multi-step forecast\n",
    "\n",
    "Task:\n",
    "* Using the iterative method produce a 12 step forecast. Save the predictions in a variable called `y_preds_lm`\n",
    "* Plot the results: predictions versus test\n",
    "* Calculate the RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoregressive_iterative_forecast(model, exog, h):\n",
    "    '''\n",
    "    h-step forecast for an autoregressive \n",
    "    model using the iterative prediction method.\n",
    "    \n",
    "    Conduct h one-step forecasts gradually\n",
    "    replacing ground truth autoregressive X \n",
    "    values with predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    ------\n",
    "    model: forecast object\n",
    "        model that has a .predict(h) interface\n",
    "        \n",
    "    exog: array-like\n",
    "        initial vector of lagged values (X)\n",
    "    \n",
    "    h: int\n",
    "        forecast horizon. assumed to be > 0\n",
    "    \n",
    "    Returns:\n",
    "    ------\n",
    "    numpy.ndarray\n",
    "        y_predictions\n",
    "    '''\n",
    "    y_preds = []\n",
    "    current_X = exog\n",
    "    for i in range(h):\n",
    "        y_pred = model.predict(current_X.reshape(1, -1))[0,0]\n",
    "        y_preds.append(y_pred)\n",
    "\n",
    "        current_X = np.roll(current_X, shift=-1)\n",
    "        current_X[-1] = y_pred\n",
    "\n",
    "    return np.array(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nn_prediction_results(model, X_train, y_train, y_test, y_preds):  \n",
    "    \n",
    "    #create series\n",
    "    fitted_values = scaler.inverse_transform(model.predict(X_train))\n",
    "    ground_truth = scaler.inverse_transform(y_train)\n",
    "    ground_truth_val = scaler.inverse_transform(y_test)\n",
    "\n",
    "    padding = np.full(len(fitted_values), np.NAN)\n",
    "\n",
    "    validation = np.concatenate([padding.reshape(-1, 1), ground_truth_val])\n",
    "    forecast = np.concatenate([padding.reshape(-1, 1), y_preds])\n",
    "\n",
    "    plt.plot(ground_truth, label='ground truth')\n",
    "    plt.plot(validation, label='test')\n",
    "    plt.plot(fitted_values, label='in-sample', linestyle='-.')\n",
    "    plt.plot(forecast, label='out-of-sample', linestyle='-.')\n",
    "    plt.plot(admit_rate.to_numpy()[12:])\n",
    "    plt.legend();\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict next 12 months and plot\n",
    "H = 12\n",
    "y_preds_lm = autoregressive_iterative_forecast(model_lm, X_test[0], h=H)\n",
    "y_preds_lm = scaler.inverse_transform(y_preds_lm.reshape(-1, 1))\n",
    "\n",
    "plot_nn_prediction_results(model_lm, X_train, y_train, y_test, y_preds_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(y_preds_lm, scaler.inverse_transform(y_test))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Training a non-linear network\n",
    "\n",
    "Task \n",
    "* Introduce a hidden layer(s) to your neural network.  Use a ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network_model(ws, n_neurons_l1=32, n_neurons_l2=64,\n",
    "                      include_layer_two=False, include_drop_out=False,\n",
    "                      drop_out_rate=0.2, lr=0.01, metrics=None):\n",
    "    if metrics is None:\n",
    "        metrics = ['mse']\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(ws,)))\n",
    "    model.add(Dense(n_neurons_l1, activation='relu'))\n",
    "    if include_layer_two:\n",
    "        model.add(Dense(n_neurons_l2, activation='relu'))\n",
    "    if include_drop_out:\n",
    "        model.add(Dropout(drop_out_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mse', \n",
    "                  optimizer=Adam(lr=lr),\n",
    "                  metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set tensorflow random seed\n",
    "tf.random.set_seed(45676)\n",
    "\n",
    "N_EPOCHS = 100\n",
    "es = EarlyStopping(monitor='loss', patience=10)\n",
    "\n",
    "#single layer nn\n",
    "mlp = get_network_model(ws=12, n_neurons_l1=5, include_layer_two=True, \n",
    "                        n_neurons_l2=32)\n",
    "\n",
    "#fit model silently\n",
    "history = mlp.fit(x=X_train, \n",
    "                    y=y_train, \n",
    "                    epochs=N_EPOCHS,\n",
    "                    verbose=0, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict next 12 months and plot\n",
    "H = 12\n",
    "y_preds_mlp = autoregressive_iterative_forecast(mlp, X_test[0], h=H)\n",
    "y_preds_mlp = scaler.inverse_transform(y_preds_mlp.reshape(-1, 1))\n",
    "\n",
    "plot_nn_prediction_results(mlp, X_train, y_train, y_test, y_preds_mlp)\n",
    "\n",
    "rmse_lm = rmse(scaler.inverse_transform(y_test), y_preds_lm)[0]\n",
    "rmse_mlp = rmse(scaler.inverse_transform(y_test), y_preds_mlp)[0]\n",
    "\n",
    "print(f'rmse lm: {rmse_lm:.2f}\\nrmse mlp: {rmse_mlp:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try changing the network parameters to see the impact on the \n",
    "#rmse relative to the linear model\n",
    "\n",
    "#set tensorflow random seed\n",
    "tf.random.set_seed(45676)\n",
    "#tf.random.set_seed(1234)\n",
    "\n",
    "N_EPOCHS = 100\n",
    "H = 12\n",
    "es = EarlyStopping(monitor='loss', patience=10)\n",
    "\n",
    "#single layer nn\n",
    "mlp = get_network_model(ws=12, \n",
    "                        n_neurons_l1=5,\n",
    "                        include_layer_two=True,\n",
    "                        n_neurons_l2=32,\n",
    "                        lr=0.1)\n",
    "\n",
    "#fit model silently\n",
    "history = mlp.fit(x=X_train, \n",
    "                    y=y_train, \n",
    "                    epochs=N_EPOCHS,\n",
    "                    verbose=0, callbacks=[es])\n",
    "\n",
    "y_preds_mlp = autoregressive_iterative_forecast(mlp, X_test[0], h=H)\n",
    "y_preds_mlp = scaler.inverse_transform(y_preds_mlp.reshape(-1, 1))\n",
    "\n",
    "rmse_lm = rmse(scaler.inverse_transform(y_test), y_preds_lm)[0]\n",
    "rmse_mlp = rmse(scaler.inverse_transform(y_test), y_preds_mlp)[0]\n",
    "\n",
    "print(f'rmse lm: {rmse_lm:.2f}\\nrmse mlp: {rmse_mlp:.2f}')\n",
    "\n",
    "plot_nn_prediction_results(mlp, X_train, y_train, y_test, y_preds_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "\n",
    "In all of the examples above we have been setting a random seed for tensorflow.  This 'suggests' that if we used a different randon number seed we would get a slightly different result. Neural networks are extremely flexible and have many parameters. This leads to one of the key challenges with neural networks - overfitting.  There are multiple ways to deal with overfitting.  In forecasting a common approach is to use an **ensemble** of models.  \n",
    "\n",
    "In an ensemble we train multiple models. \n",
    "\n",
    "## Training an ensemble\n",
    "\n",
    "We will train an ensemble of neural networks that mimic a linear model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_ensemble(n_models):\n",
    "    models = []\n",
    "    url = '/input'\n",
    "    for n in range(n_models):\n",
    "        model_n = tf.keras.models.load_model(f'{url}/ensemble_model_{n}.h5')\n",
    "        models.append(model_n)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set random seed so that ensemble can be repeated.\n",
    "tf.random.set_seed(1085)\n",
    "\n",
    "N_MODELS = 20\n",
    "N_EPOCHS = 100\n",
    "WINDOW_SIZE = 12\n",
    "es = EarlyStopping(monitor='loss', patience=10)\n",
    "\n",
    "#I've pretrained 50 models you can load them from file if wanted.\n",
    "LOAD_FROM_FILE = False\n",
    "\n",
    "if LOAD_FROM_FILE:\n",
    "    #it will take a few seconds to load.\n",
    "    models = load_pretrained_ensemble(N_MODELS)\n",
    "else:\n",
    "    models = []\n",
    "    for n in range(N_MODELS):\n",
    "        #single layer nn\n",
    "        model_n = get_linear_model(WINDOW_SIZE)\n",
    "\n",
    "        #fit model silently\n",
    "        history = model_n.fit(x=X_train, \n",
    "                              y=y_train, \n",
    "                              epochs=N_EPOCHS,\n",
    "                              verbose=0, \n",
    "                              callbacks=[es], \n",
    "                              batch_size=32)\n",
    "\n",
    "        #this will overwrite pre-trained models.\n",
    "        model_n.save(f'input/ensemble_model_{n}.h5')\n",
    "        models.append(model_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions in an ensemble\n",
    "\n",
    "In an ensemble we predict in a loop. In python this is straightfoward as we simply loop through the models we have trained and call `autoregressive_iterative_forecast`. Store the predictions of each forecast in a python `list`  called `e_preds`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code will take a few seconds to execute\n",
    "H = 12\n",
    "e_preds = []\n",
    "for model in models:\n",
    "    y_preds = autoregressive_iterative_forecast(model, X_test[0], h=H)\n",
    "    e_preds.append(y_preds)\n",
    "    \n",
    "e_preds = np.array(e_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverse transform the data and calculate the median and 0.025 and 0.975 percentiles of the point forecasts\n",
    "\n",
    "Remember we can use `scaler.inverse_transform()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_preds = np.asarray(e_preds)\n",
    "e_preds_tran = scaler.inverse_transform(e_preds).T\n",
    "y_preds_mdn = np.percentile(e_preds_tran.T, 50, axis=0)\n",
    "y_preds_2_5 = np.percentile(e_preds_tran.T, 2.5, axis=0)\n",
    "y_preds_97_5 = np.percentile(e_preds_tran.T, 97.5, axis=0)\n",
    "y_preds_mdn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the individual forecasts and the median\n",
    "\n",
    "fig,ax = plt.subplots(1, 2, sharey=True, figsize=(12, 4))\n",
    "ax[0].plot(e_preds_tran)\n",
    "ax[0].plot(scaler.inverse_transform(y_test), label='test', linestyle='--', \n",
    "         color='red')\n",
    "ax[0].plot(y_preds_mdn, label='median', linestyle='-', color='black')\n",
    "ax[0].legend()\n",
    "ax[0].set_title(f'Point forecasts: {N_MODELS} models')\n",
    "\n",
    "ax[1].plot(scaler.inverse_transform(y_test), label='test', linestyle='--', \n",
    "         color='red')\n",
    "ax[1].plot(y_preds_mdn, label='median', linestyle='-', color='black')\n",
    "ax[1].plot(y_preds_2_5, label='0.025 percentile', linestyle='-.', color='black')\n",
    "ax[1].plot(y_preds_97_5, label='0.975 percentile', linestyle='--', color='black')\n",
    "#ax[1].plot(y_preds_lm, label='original lmforecast', linestyle='--', color='green')\n",
    "ax[1].set_title(f'Middle 95% of point forecasts ')\n",
    "ax[1].legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_lm = rmse(scaler.inverse_transform(y_test), y_preds_lm)[0]\n",
    "rmse_mdn = rmse(scaler.inverse_transform(y_test), y_preds_mdn)[0]\n",
    "\n",
    "print(f'rmse lm: {rmse_lm:.2f}\\nrmse ensemble: {rmse_mdn:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_25 = rmse(scaler.inverse_transform(y_test), y_preds_2_5)[0]\n",
    "rmse_75 = rmse(scaler.inverse_transform(y_test), y_preds_97_5)[0]\n",
    "print(f'95% of linear models will have rmse between: {rmse_75:.2f} - {rmse_25:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So were the original linear model results just a fluke due to the choice of seed?  \n",
    "Let's try another seed to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set tensorflow random seed for repeatability\n",
    "#try multiple seeds\n",
    "tf.random.set_seed(1066)\n",
    "\n",
    "N_EPOCHS = 100\n",
    "es = EarlyStopping(monitor='loss', patience=10)\n",
    "\n",
    "#compile the tf model\n",
    "model_lm = get_linear_model(ws=12, metrics=['mae'])\n",
    "\n",
    "#fit model silently\n",
    "history = model_lm.fit(x=X_train, \n",
    "                      y=y_train, \n",
    "                      epochs=N_EPOCHS,\n",
    "                      verbose=0,\n",
    "                      batch_size=32,\n",
    "                      callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict next 12 months and plot\n",
    "H = 12\n",
    "y_preds_lm2 = autoregressive_iterative_forecast(model_lm, X_test[0], h=H)\n",
    "y_preds_lm2 = scaler.inverse_transform(y_preds_lm2.reshape(-1, 1))\n",
    "\n",
    "plot_nn_prediction_results(model_lm, X_train, y_train, y_test, y_preds_lm2)\n",
    "\n",
    "rmse_lm = rmse(scaler.inverse_transform(y_test), y_preds_lm)[0]\n",
    "rmse_lm2 = rmse(scaler.inverse_transform(y_test), y_preds_lm2)[0]\n",
    "\n",
    "print(f'rmse lm: {rmse_lm:.2f}\\nrmse lm2: {rmse_lm2:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Create an ensemble of non-linear models.\n",
    "\n",
    "The two layer model appears to be more accurate than the simple linear regression model and its ensemble counterpart.\n",
    "\n",
    "Task: \n",
    "\n",
    "* Create an ensemble of 20 models.\n",
    "* Use the `get_network_model()` and its parameters to compile your model.\n",
    "* Optional: save your models to file. (recommended)\n",
    "* Forecast the next 12 periods.\n",
    "* Calculate the RMSE of the forecast.\n",
    "\n",
    "\n",
    "Hints:\n",
    "\n",
    "* You have **all of the code** you need to complete this task!\n",
    "* Remember to back transform your forecasts\n",
    "* Use the median of the ensemble.\n",
    "* Look carefully at the previous ensemble example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set tensorflow random seed for repeatability\n",
    "tf.random.set_seed(1066)\n",
    "\n",
    "N_MODLES = 20\n",
    "N_EPOCHS = 100\n",
    "H = 12\n",
    "es = EarlyStopping(monitor='loss', patience=10)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "models = []\n",
    "for n in range(N_MODELS):\n",
    "    #multi-layer model\n",
    "    model_n = get_network_model(ws=12, \n",
    "                                n_neurons_l1=5,\n",
    "                                include_layer_two=True,\n",
    "                                n_neurons_l2=32,\n",
    "                                lr=0.1)\n",
    "\n",
    "    #fit model silently\n",
    "    history = model_n.fit(x=X_train, \n",
    "                          y=y_train, \n",
    "                          epochs=N_EPOCHS,\n",
    "                          verbose=0,\n",
    "                          batch_size=BATCH_SIZE)\n",
    "\n",
    "    #this will overwrite pre-trained models.\n",
    "    model_n.save(f'output/mlp_ensemble_{n}.h5')\n",
    "    models.append(model_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code will take a few seconds to execute\n",
    "H = 12\n",
    "e_preds = []\n",
    "for model in models:\n",
    "    y_preds = autoregressive_iterative_forecast(model, X_test[0], h=H)\n",
    "    e_preds.append(y_preds)\n",
    "    \n",
    "e_preds = np.array(e_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_preds = np.asarray(e_preds)\n",
    "e_preds_tran = scaler.inverse_transform(e_preds).T\n",
    "y_preds_mdn = np.percentile(e_preds_tran.T, 50, axis=0)\n",
    "y_preds_2_5 = np.percentile(e_preds_tran.T, 2.5, axis=0)\n",
    "y_preds_97_5 = np.percentile(e_preds_tran.T, 97.5, axis=0)\n",
    "y_preds_mdn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the individual forecasts and the median\n",
    "\n",
    "fig,ax = plt.subplots(1, 2, sharey=True, figsize=(12, 4))\n",
    "ax[0].plot(e_preds_tran)\n",
    "ax[0].plot(scaler.inverse_transform(y_test), label='test', linestyle='--', \n",
    "         color='red')\n",
    "ax[0].plot(y_preds_mdn, label='median', linestyle='-', color='black')\n",
    "ax[0].legend()\n",
    "ax[0].set_title(f'Point forecasts: {N_MODELS} models')\n",
    "\n",
    "ax[1].plot(scaler.inverse_transform(y_test), label='test', linestyle='--', \n",
    "         color='red')\n",
    "ax[1].plot(y_preds_mdn, label='median', linestyle='-', color='black')\n",
    "ax[1].plot(y_preds_2_5, label='0.025 percentile', linestyle='-.', color='black')\n",
    "ax[1].plot(y_preds_97_5, label='0.975 percentile', linestyle='--', color='black')\n",
    "#ax[1].plot(y_preds_lm, label='original lmforecast', linestyle='--', color='green')\n",
    "ax[1].set_title(f'Middle 95% of point forecasts ')\n",
    "ax[1].legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_lm = rmse(scaler.inverse_transform(y_test), y_preds_mlp)[0]\n",
    "rmse_mdn = rmse(scaler.inverse_transform(y_test.T), y_preds_mdn)[0]\n",
    "\n",
    "print(f'rmse lm: {rmse_mlp:.2f}\\nrmse ensemble: {rmse_mdn:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra exercise.\n",
    "* How would you use a ensemble method with a model that predicts a vector?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
