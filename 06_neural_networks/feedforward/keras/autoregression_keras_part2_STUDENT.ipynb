{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoregressive models using a feedforward neural network \n",
    "## PART 2: Applying the methods to health care time series\n",
    "\n",
    "In this notebook we will use a feedforward neural network to fit a single and ensemble linear and non-linear models to real time series data. \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "1. Most of the work we will do is data manipulation: preprocessing data and making sure it is the right shape for the neural networks.\n",
    "\n",
    "2. The ensemble learning method can be computationally expensive.  We have included some pre-trained models that can be loaded from file if needed.\n",
    "</div>\n",
    "\n",
    "---\n",
    "**LEARNING OBJECTIVES**\n",
    "\n",
    "* Learn how to apply feedforward neural networks to real health data.\n",
    "* Methods to preprocess nn input data.\n",
    "* Recognise the stochastic nature of neural network training\n",
    "* Use a ensemble of neural networks to provide a more reliable point forecast\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python dependencies\n",
    "\n",
    "It is recommended that you use the forecasting course conda environment provided. We are again going to implement neural networks using `tensorflow` and '`keras`. You should be using at least `tensorflow` version `2.1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# tensorflow imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting emergency admissions in England\n",
    "\n",
    "We will now use feedforward neural networks to predict the number of monthly emergency admissions in England. \n",
    "\n",
    "## Load the data\n",
    "\n",
    "**Task**:\n",
    "* Execute the code below to read the emergency admissions data into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/health-data-science-OR/data/master' \\\n",
    "        + '/em_admits_ts.csv'\n",
    "em_admits = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_admits.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_admits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Preprocessing \n",
    "\n",
    "### 2.3.1 Datetime format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the the `month_year` column in `em_admits` holds a string an invalid date format e.g. 'Aug-10'.  Pandas cannot handle this as-is because '10' could refer to any century!  So let's do a bit of preprocessing to get it into a valid datetime format.\n",
    "\n",
    "\n",
    "*Optional Task:*\n",
    "* Take some time to understand the code that preprocesses the dates.  This is real health data and it is likely you will need to deal with formatting issues as experienced here.\n",
    "\n",
    "First we will format the string to something pandas can parse i.e. 'Aug 2010'.  Then we will call the `pd.to_datetime()` function to parse the string and return a `datetime`.  We will assign the result to our dataframe's index and set the freq to monthly start 'MS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = em_admits['month_year'].str[:3] + ' 20' \\\n",
    "    + em_admits['month_year'].str[-2:]\n",
    "date_str.name = 'date'\n",
    "em_admits = em_admits.set_index(pd.to_datetime(date_str))\n",
    "em_admits.index.freq = 'MS'\n",
    "em_admits = em_admits.drop(columns=['month_year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_admits.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be forecasting the last 12 months of the series.  Let's take a look at the training data (being careful to exclude the last 12 months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_length = 12\n",
    "em_admits[:len(em_admits)-holdout_length].plot(figsize=(12,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calender adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is monthly data so a useful preprocessing step is to transform the data into a daily rate by dividing by the number of days in the month. When we plot this the troughs we saw in Feb each year disappear.\n",
    "\n",
    "**execute the code below which:**:\n",
    "* Calculates the average admissions per day series\n",
    "* Plots the training data (holding back 12 months for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admit_rate = em_admits['em_admits'] / em_admits.index.days_in_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admit_rate[:len(admit_rate)-12].plot(figsize=(12,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 1**: Convert the time series to format suitable for supervised learning.\n",
    "\n",
    "The function `sliding_window` has been provided below for you to create your training data.\n",
    "\n",
    "**Task**:\n",
    "* Using a sliding window approach convert the time series into a tabular format. \n",
    " * Use a window size of 12 and assume you are predicting a scalar value of y (1-step ahead).\n",
    "* Conduct a train test split holding back 12 windows as a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(train, window_size=2, horizon=1):\n",
    "    '''\n",
    "    sliding window.\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    train: array-like\n",
    "        training data for time series method\n",
    "    \n",
    "    window_size: int, optional (default=2)\n",
    "        lookback - how much lagged data to include.\n",
    "        \n",
    "    horizon: int, optional (default=1)\n",
    "        number of observations ahead to predict\n",
    "            \n",
    "    Returns:\n",
    "        array-like, array-like\n",
    "    \n",
    "        preprocessed X, preprocessed Y\n",
    "    '''\n",
    "    tabular_X = []\n",
    "    tabular_y = []\n",
    "    \n",
    "    for i in range(0, len(train) - window_size - horizon):\n",
    "        X_train = train[i:window_size+i]\n",
    "        y_train = train[i+window_size+horizon-1]\n",
    "        tabular_X.append(X_train)\n",
    "        tabular_y.append(y_train)\n",
    "       \n",
    "    return np.asarray(tabular_X), np.asarray(tabular_y).reshape(-1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here...\n",
    "\n",
    "# get data in tabular format for NN\n",
    "#X_data, y_data = sliding_window(...)\n",
    "\n",
    "#X_train, y_train, X_test, y_test = ... train test split code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling the features and target to be between -1 and 1\n",
    "In many machine learning applications data are scaled to be between 0 and 1. For neural network forecasting, *Ord, Fildes and Kourentzes (2017)* recommend scaling to be between -1 and 1.  This is what we will do here.  To do the scaling we will use\n",
    "\n",
    "```python\n",
    "sklearn.preprocessing.MinMaxScaler\n",
    "```\n",
    "\n",
    "> Execute the code below to transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# scale on training data\n",
    "scaler.fit(admit_rate.iloc[:-12].to_numpy().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the shape of X_train\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peak at first sample in X_train\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_arrays_of_X(X_to_scale, scaler):\n",
    "    \"\"\"\n",
    "    Takes a 2d array and transforms data using a sklearn scaler.  \n",
    "    This is executed in a loop as the sklearn scalers have a strict \n",
    "    interface for the dimensions of X.\n",
    "    \n",
    "    Params:\n",
    "    ------\n",
    "    X_to_scale: np.ndarray\n",
    "        Sliding windows of X data\n",
    "        \n",
    "    scaler: MinMaxScaler\n",
    "        Fitted scaler used to transform X_train.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    np.ndarray \n",
    "        Dimensions are the same as X_train.\n",
    "    \"\"\"\n",
    "    X_scaled = []\n",
    "    \n",
    "    # scale each sample separately\n",
    "    for sample in X_to_scale:\n",
    "        # scale the sample\n",
    "        scaled_sample = scaler.transform(sample.reshape(-1, 1))\n",
    "        \n",
    "        # store the sample in a list\n",
    "        X_scaled.append(scaled_sample)\n",
    "    \n",
    "    # return the scaled data as np.ndarray with dimensions from input\n",
    "    return np.array(X_scaled).reshape(len(X_scaled), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale all data\n",
    "y_train = scaler.transform(y_train.reshape(-1, 1))\n",
    "X_train = scale_arrays_of_X(X_train, scaler)\n",
    "X_test = scale_arrays_of_X(X_test, scaler)\n",
    "y_test = scaler.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is the shape of X_train correct? yes.\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peak at first sample after transform.\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 2**: A Linear regression model benchmark\n",
    "\n",
    "The first model we will try is the linear model. Its will serve as our neural network baseline.  (In practice we would also check this is better than a naive method such as seasonal naive).\n",
    "\n",
    "## Exercise 2a Train the model\n",
    "\n",
    "**Task:**\n",
    "* Using `Keras`, construct a neural network that mimics a simple linear regression model (see previous notebook).  \n",
    "* Optional: To get comparable results, set the tensorflow random number seed to 1234\n",
    "* Train the model for 100 epochs.\n",
    "* Optionally you can use an early stopping callback with patience set to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here ...\n",
    "\n",
    "def get_linear_model(ws):\n",
    "    '''\n",
    "    Sequential Keras model that minics\n",
    "    AR linear model. \n",
    "    \n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2b. Generate and evaluate a multi-step forecast\n",
    "\n",
    "**Task:**\n",
    "* Using the iterative method produce a 12 step forecast. Save the predictions in a variable called `y_preds_lm`\n",
    "* Calculate the RMSE\n",
    "* Optional: Plot the results -> predictions versus validation.\n",
    "\n",
    "**Hints:**\n",
    "* A function `autoregressive_iterative_forecast` is provided below.  (you could use this function or write your own if you prefer!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoregressive_iterative_forecast(model, exog, h):\n",
    "    '''\n",
    "    h-step forecast for an autoregressive \n",
    "    model using the iterative prediction method.\n",
    "    \n",
    "    Conduct h one-step forecasts gradually\n",
    "    replacing ground truth autoregressive X \n",
    "    values with predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    ------\n",
    "    model: forecast object\n",
    "        model that has a .predict(h) interface\n",
    "        \n",
    "    exog: array-like\n",
    "        initial vector of lagged values (X)\n",
    "    \n",
    "    h: int\n",
    "        forecast horizon. assumed to be > 0\n",
    "    \n",
    "    Returns:\n",
    "    ------\n",
    "    numpy.ndarray\n",
    "        y_predictions\n",
    "    '''\n",
    "    y_preds = []\n",
    "    current_X = exog\n",
    "    for i in range(h):\n",
    "        y_pred = model.predict(current_X.reshape(1, -1))[0,0]\n",
    "        y_preds.append(y_pred)\n",
    "\n",
    "        current_X = np.roll(current_X, shift=-1)\n",
    "        current_X[-1] = y_pred\n",
    "\n",
    "    return np.array(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 3:** Training a non-linear deep network\n",
    "\n",
    "Now that you have got the basic structure and mechanics of the code you need for forecasting let's build a more complex model and compare the RMSE on the validation set to your simple linear model.\n",
    "\n",
    "**Task:** \n",
    "* Create a new neural network model with 2 hidden layers\n",
    "* Try 32 and 64 neurons for layer 1 and 2 respectively\n",
    "* Use a ReLU activation function.\n",
    "* Use the Adam optimiser with a learning rate of 0.01\n",
    "* Predict the next 12 months ahead\n",
    "* Calculate the RMSE\n",
    "\n",
    "**Hints:**\n",
    "* Feel free to experiment with the number of hidden layers, neurons and learning rate.\n",
    "* Perhaps try a dropout layer(s) if you feel your model is overfitting.\n",
    "* Set a tensorflow random seed if you want to be able to reproduce your results e.g. 45676\n",
    "* To organise your code you may want to create a function that creates the Keras model. This could, for example, be called `get_network_model`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "\n",
    "In all of the examples above we have been setting a random seed for tensorflow.  This 'suggests' that if we used a different randon number seed we would get a slightly different result (this is due to both random initialisation of weights/biases and stochastic gradient descent). Neural networks are extremely flexible and have many parameters. This leads to one of the key challenges with neural networks - overfitting.  There are multiple ways to deal with overfitting.  In forecasting a common approach is to use an **ensemble** of models.  \n",
    "\n",
    "In an ensemble we train multiple models. \n",
    "\n",
    "## Training an ensemble\n",
    "\n",
    "We will train an ensemble of neural networks that mimic a linear model.  \n",
    "\n",
    "The code below has been provided for you to work through.\n",
    "\n",
    "* We set some parameters e.g. number of models in an the ensemble: 20 to 30 should be plenty.\n",
    "* We use a python loop to create and train each model and store the model in a python list.\n",
    "    * Optionally we can save the models to file and load pre-trained versions at a later date.\n",
    "* To predict we the need to loop through the collection of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_ensemble(n_models):\n",
    "    '''\n",
    "    Load the pre-trained ensemble models (only use if they exist!)\n",
    "    '''\n",
    "    models = []\n",
    "    url = '/input'\n",
    "    for n in range(n_models):\n",
    "        model_n = tf.keras.models.load_model(f'{url}/ensemble_model_{n}.h5')\n",
    "        models.append(model_n)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script to train the models.\n",
    "\n",
    "################# Parameters for the ensemble #################################\n",
    "#set random seed so that ensemble can be repeated.\n",
    "tf.random.set_seed(1085)\n",
    "\n",
    "#number of models to create...\n",
    "N_MODELS = 20\n",
    "\n",
    "#max no. of epochs for training of each model.\n",
    "N_EPOCHS = 100\n",
    "\n",
    "#no. of autoregressive lags\n",
    "WINDOW_SIZE = 12\n",
    "\n",
    "#early stopping reguluarization\n",
    "es = EarlyStopping(monitor='loss', patience=10)\n",
    "\n",
    "#I've pretrained 50 models you can load them from file if wanted.\n",
    "LOAD_FROM_FILE = False\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "if LOAD_FROM_FILE:\n",
    "    #it will take a few seconds to load.\n",
    "    models = load_pretrained_ensemble(N_MODELS)\n",
    "else:\n",
    "    models = []\n",
    "    for n in range(N_MODELS):\n",
    "        # single layer nn\n",
    "        model_n = get_linear_model(WINDOW_SIZE)\n",
    "\n",
    "        # fit model silently (verbose=0)\n",
    "        history = model_n.fit(x=X_train, \n",
    "                              y=y_train, \n",
    "                              epochs=N_EPOCHS,\n",
    "                              verbose=0, \n",
    "                              callbacks=[es], \n",
    "                              batch_size=32)\n",
    "\n",
    "        # this will overwrite pre-trained models.\n",
    "        model_n.save(f'input/ensemble_model_{n}.h5')\n",
    "        models.append(model_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions in an ensemble\n",
    "\n",
    "In an ensemble, we predict in a loop. In python this is straightfoward as we simply loop through the models we have trained and call `autoregressive_iterative_forecast`. We will store the predictions of each forecast in a python `list`  called `e_preds`\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "In an ensemble we end up with a distribution of forecasts!  For point forecasts we could then take the median of the forecasts.  We can also get a measure of variability in the forecasts by calculating the quantiles. \n",
    "    \n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the forecasts\n",
    "# this code will take a few seconds to execute\n",
    "H = 12\n",
    "e_preds = []\n",
    "for model in models:\n",
    "    y_preds = autoregressive_iterative_forecast(model, X_test[0], h=H)\n",
    "    e_preds.append(y_preds)\n",
    "    \n",
    "e_preds = np.array(e_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverse transform the data and calculate the median and 0.025 and 0.975 percentiles of the point forecasts\n",
    "\n",
    "Remember we can use `scaler.inverse_transform()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_preds = np.asarray(e_preds)\n",
    "e_preds_tran = scaler.inverse_transform(e_preds).T\n",
    "y_preds_mdn = np.percentile(e_preds_tran.T, 50, axis=0)\n",
    "y_preds_2_5 = np.percentile(e_preds_tran.T, 2.5, axis=0)\n",
    "y_preds_97_5 = np.percentile(e_preds_tran.T, 97.5, axis=0)\n",
    "y_preds_mdn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the individual forecasts and the median\n",
    "\n",
    "fig,ax = plt.subplots(1, 2, sharey=True, figsize=(12, 4))\n",
    "ax[0].plot(e_preds_tran)\n",
    "ax[0].plot(scaler.inverse_transform(y_test), label='test', linestyle='--', \n",
    "         color='red')\n",
    "ax[0].plot(y_preds_mdn, label='median', linestyle='-', color='black')\n",
    "ax[0].legend()\n",
    "ax[0].set_title(f'Point forecasts: {N_MODELS} models')\n",
    "\n",
    "ax[1].plot(scaler.inverse_transform(y_test), label='test', linestyle='--', \n",
    "         color='red')\n",
    "ax[1].plot(y_preds_mdn, label='median', linestyle='-', color='black')\n",
    "ax[1].plot(y_preds_2_5, label='0.025 percentile', linestyle='-.', color='black')\n",
    "ax[1].plot(y_preds_97_5, label='0.975 percentile', linestyle='--', color='black')\n",
    "#ax[1].plot(y_preds_lm, label='original lmforecast', linestyle='--', color='green')\n",
    "ax[1].set_title(f'Middle 95% of point forecasts ')\n",
    "ax[1].legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_lm = rmse(scaler.inverse_transform(y_test), y_preds_lm)[0]\n",
    "rmse_mdn = rmse(scaler.inverse_transform(y_test), y_preds_mdn)[0]\n",
    "\n",
    "print(f'rmse lm: {rmse_lm:.2f}\\nrmse ensemble: {rmse_mdn:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_25 = rmse(scaler.inverse_transform(y_test), y_preds_2_5)[0]\n",
    "rmse_75 = rmse(scaler.inverse_transform(y_test), y_preds_97_5)[0]\n",
    "print(f'95% of linear models will have rmse between: {rmse_75:.2f} - {rmse_25:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Is the ensemble approach useful?  What does it tell us about our original linear model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Create an ensemble of non-linear models.\n",
    "\n",
    "Is the two layer model more accurate than the simple linear regression model and its ensemble counterpart?\n",
    "\n",
    "**Task:** \n",
    "\n",
    "* Create an ensemble of 20 models.\n",
    "* Each model should be based on your solution to exercise 2 (e.g. a neural network with 2 hidden layers)\n",
    "* Optional: save your models to file. (recommended)\n",
    "* Forecast the next 12 periods.\n",
    "* Calculate the RMSE of the forecast.\n",
    "\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "* You have **all of the code** you need to complete this task!\n",
    "* Remember to back transform your forecasts\n",
    "* Use the median of the ensemble.\n",
    "* Look carefully at the previous ensemble example.\n",
    "\n",
    "**Questions**\n",
    "* Which out of the simple linear, multi-layer and ensemble models do you think is best in this instance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Extra exercise for you to think about.\n",
    "* How would you use a ensemble method with a model that predicts a vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of lab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
