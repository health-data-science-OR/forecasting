{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoregressive model using a feedforward neural network (pytorch implementation)\n",
    "\n",
    "In this notebook we will use a feedforward neural network to fit a linear model to time series data. \n",
    "\n",
    "> In this notebook all models are implemented with pytorch.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "1. Pytorch has an excellent, but slightly more involved interface than Keras provides for Tensorflow.  \n",
    "    \n",
    "2. The data preprocessing requirements for a NN are similar to those for Keras and TensorFlow, but there are additional steps needed regarding mini batches and make sure data are tensors.\n",
    "\n",
    "2. Forecasting **h-steps** ahead uses the same approach as Keras/Tensorflow, but there are subtle differnces in the code.\n",
    "</div>\n",
    "\n",
    "---\n",
    "**LEARNING OBJECTIVES**\n",
    "\n",
    "* Use a NN to mimic a linear model using PyTorch.\n",
    "* Train a deep learning model implemented using PyTorch.\n",
    "* Generate h-step forecasts using an iterative approach\n",
    "* Generate h-step forecast using a direct modelling approach\n",
    "* Construct a deep feedforward neural network for forecasting\n",
    "* Use a ensemble of neural networks to forecast\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Python dependencies\n",
    "\n",
    "It is recommended that you use the forecasting course conda environment provided for this work. We are going to implement neural networks using `pytorch`. You should be using at least `pytorch` version `1.4.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import time\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reminder: The forecasting process for AR\n",
    "\n",
    "1. Select $l$ the number of autoregressive lags and forecast horizon $h$\n",
    "2. Preprocess the data into tabular form [[$lag_1, lag_2, ... lag_l$], [$y_t$]]\n",
    "3. Train the NN model using the tabular data\n",
    "4. Iteratively forecast 1-step ahead gradually replacing ground truth observations with predictions.\n",
    "\n",
    "\n",
    "### 2.1 Synthetic data without noise\n",
    "\n",
    "Given the extra complexities of forecasting using OLS we will use simple synthetic data before exploring real healthcare data. The synthetic data we wil use is a cosine.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(200)\n",
    "ts_data = np.cos(0.2 * t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ts_data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Preprocess the time series into tabular autoregressive form\n",
    "\n",
    "An autoregressive model consists of $l$ lags of the time series. \n",
    "\n",
    "An easy way to think about the form of the data for a autoregressive OLS model is as a table of variables.  The first $l$ columns are the lags (the independent predictor variables) and the final column is $y$ at time $t$ ($y_t$) that is, the target/dependent variable.  \n",
    "\n",
    "We there need to manipulate the time series so that is now in that format.  More precisely for each row we need: \n",
    "\n",
    "**A vector presenting the lags at time t**\n",
    "* $X_t = $ [$lag_{t-l}, ... lag_{t-2}, lag_{t-1}$]\n",
    "\n",
    "**A scalar value representing y at time t:**\n",
    "* $y_t$\n",
    "\n",
    "For training we need a vector of rows ($X_t$) and vector of target $y_t$. e.g.\n",
    "\n",
    "```python\n",
    "X_train = [X_1, X_2, X_3, ...,  X_t]\n",
    "\n",
    "y_train = [y_1, y_2, y_3, ..., y_t]\n",
    "```\n",
    "---\n",
    "\n",
    "The function `sliding_window` illustrates how to preprocess time series data into tabular form  in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(train, window_size=2, horizon=1):\n",
    "    '''\n",
    "    sliding window.\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    train: array-like\n",
    "        training data for time series method\n",
    "    \n",
    "    window_size: int, optional (default=2)\n",
    "        lookback - how much lagged data to include.\n",
    "        \n",
    "    horizon: int, optional (default=1)\n",
    "        number of observations ahead to predict\n",
    "            \n",
    "    Returns:\n",
    "        array-like, array-like\n",
    "    \n",
    "        preprocessed X, preprocessed Y\n",
    "    '''\n",
    "    tabular_X = []\n",
    "    tabular_y = []\n",
    "    \n",
    "    for i in range(0, len(train) - window_size - horizon):\n",
    "        X_train = train[i:window_size+i]\n",
    "        y_train = train[i+window_size+horizon-1]\n",
    "        tabular_X.append(X_train)\n",
    "        tabular_y.append(y_train)\n",
    "       \n",
    "    return np.asarray(tabular_X), np.asarray(tabular_y).reshape(-1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensors(*arrays):\n",
    "    results = ()\n",
    "    for a in arrays:\n",
    "        results += torch.FloatTensor(a),\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(X, y, batch_size=32):\n",
    "    '''\n",
    "    Set up train data as a TensorDataSet\n",
    "    '''\n",
    "    tensor_data = TensorDataset(torch.FloatTensor(X),\n",
    "                                torch.FloatTensor(y))\n",
    "\n",
    "    return DataLoader(tensor_data, \n",
    "                      batch_size=batch_size, \n",
    "                      shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_train_test_split(*arrays, train_size, as_tensors=True):\n",
    "    '''\n",
    "    time series train test split\n",
    "    \n",
    "    Parameters:\n",
    "    X: array-like\n",
    "        X data\n",
    "    y_data \n",
    "    '''\n",
    "    results = ()\n",
    "    for a in arrays:\n",
    "        if as_tensors:\n",
    "            results += to_tensors(a[:train_size], a[train_size:])\n",
    "        else:\n",
    "            results += a[:train_size], a[train_size:]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 5\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# preprocess time series into a supervised learning problem\n",
    "X_data, y_data = sliding_window(ts_data, window_size=WINDOW_SIZE)\n",
    "\n",
    "# train test split\n",
    "train_size = int(len(y_data) * (2/3))\n",
    "X_train, X_test, y_train, y_test = ts_train_test_split(X_data, \n",
    "                                                y_data,\n",
    "                                                train_size=train_size,\n",
    "                                                as_tensors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Create the PyTorch model.\n",
    "\n",
    "After preprocessing the data, we need to create our PyTorch model.  The first model we create will mimic the linear autoregressive model we built using an instance of `OLS`. This means we have neural network with a single layer with `window_size` inputs ans a single output.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model on PyTorch nn.Module class\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, window_size):\n",
    "        # Inherit parent (nn.module) methods using super init\n",
    "        super(LinearModel, self).__init__()\n",
    "        # Linear model only has a single layer \n",
    "        # window_size input containing our lags\n",
    "        # a Linear object is the same as Keras' Dense layer.\n",
    "        self.layer1 = nn.Linear(in_features=window_size, \n",
    "                                out_features=1, \n",
    "                                bias=True)   \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass data through net. \n",
    "        y_pred = self.layer1(x)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, optimizer, criterion, n_epochs, \n",
    "        X_train, y_train, X_test=None, y_test=None, \n",
    "        batch_size=32, verbose=0):\n",
    "    '''\n",
    "    train the pytorch model \n",
    "    \n",
    "    Parameters:\n",
    "    ------\n",
    "    model: torch.nn.module\n",
    "        PyTorch Neural Network Model implements .forward()\n",
    "    \n",
    "    optimizer: torch.optim.Optimizer\n",
    "        PyTorch optimization engine e.g. Adam\n",
    "        \n",
    "    criterion: torch.nn.criterion\n",
    "        PyTorch criterion e.g. MSELoss \n",
    "        \n",
    "    n_epochs: int\n",
    "        Number of epochs to train\n",
    "        \n",
    "    X_train: Tensor\n",
    "        x training matrix 2D\n",
    "        \n",
    "    y_train: Tensor\n",
    "        y training vector\n",
    "        \n",
    "    X_test: Tensor, optional (default=None)\n",
    "        x test matrix 2D\n",
    "        \n",
    "    y_test: Tensor, optional (default=None)\n",
    "        y test vector\n",
    "        \n",
    "    batch_size: int, optional (default=32)\n",
    "        Size of the mini batches used in training\n",
    "        \n",
    "    verbose: int, optional (default=0)\n",
    "        0 == no output during training\n",
    "        1 == output loss every 10 epochs \n",
    "        (includes validation loss if X_test, y_test included)\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "        dict\n",
    "        training and validation loss history. keys are\n",
    "        'loss' and 'val_loss'\n",
    "    '''\n",
    "    PRINT_STEPS = 10\n",
    "\n",
    "    # Set up lists for loss\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    history = {'loss':[],\n",
    "               'val_loss':[]}\n",
    "\n",
    "    # create the mini-batches\n",
    "    train_loader = get_data_loader(X_train, \n",
    "                               y_train, \n",
    "                               batch_size=batch_size)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Loop through required number of epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        # Train model (using batches): Switch to training mode\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            \n",
    "            y_pred = model.forward(batch[0])\n",
    "            loss = criterion(y_pred, batch[1].reshape(y_pred.shape[0], -1))\n",
    "\n",
    "            # Zero gradients, perform a backward pass,  \n",
    "            # and update the weights. \n",
    "            optimizer.zero_grad() \n",
    "            loss.backward() \n",
    "            optimizer.step() \n",
    "\n",
    "        # Get results for complete training set: Switch to evaluation mode\n",
    "        model.eval()\n",
    "        y_pred_train = model.forward(X_train)\n",
    "        history['loss'].append(criterion(y_pred_train, \n",
    "                                         y_train.reshape(y_pred_train.shape[0], \n",
    "                                                         -1)).detach())\n",
    "\n",
    "        if not X_test is None:\n",
    "            # Get results for test set\n",
    "            y_pred_test = model.forward(X_test)\n",
    "            history['val_loss'].append(criterion(y_pred_test, \n",
    "                                                 y_test.reshape(y_pred_test.shape[0], \n",
    "                                                                -1)).detach())\n",
    "\n",
    "        # Print loss & accuracy every 10 epochs (prt last item of results list)\n",
    "        if verbose == 1:\n",
    "            if (epoch+1) % PRINT_STEPS == 0:\n",
    "                print(f'Epoch {epoch+1}. ', end='')\n",
    "                print(f\"Train accuracy {history['loss'][-1]: 0.3f}. \", end='')\n",
    "                print(f\"Test accuracy {history['val_loss'][-1]: 0.3f}.\")\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    if verbose == 1:\n",
    "        print(f'Training time {duration:.2f}s')\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "\n",
    "N_EPOCHS = 100\n",
    "\n",
    "# Create model\n",
    "model = LinearModel(WINDOW_SIZE)\n",
    "# Set loss \n",
    "criterion = nn.MSELoss()\n",
    "# Set optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "history = fit(model, optimizer, criterion, N_EPOCHS, \n",
    "              X_train, y_train, X_test=X_test, y_test=y_test, \n",
    "              batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(N_EPOCHS), history['loss'], label='loss')\n",
    "plt.plot(range(N_EPOCHS), history['val_loss'], label='val_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Forecasting 1 step ahead\n",
    "\n",
    "To forecast 1-step ahead we use the just need to pass tbe first element of the `X_test` tensor to the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pred = model(X_test[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'1-step forecast: {pred:10.4f}')\n",
    "print(f'ground trust value: {y_test[0]:6.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Forecast h periods ahead using the iterative method.\n",
    "\n",
    "**We have trained our `NN` model to predict 1-step**. When forecasting 2 or more steps ahead we still only have five ground truth observations ($lag_1$ ... $lag_5$). This means that when forecasting h-steps ahead we need to do this in a loop where we iteratively replace our ground truth observations with our predictions.\n",
    "\n",
    "There's an easy way to do this in pytorch using the `torch.roll(tensor, shifts)` function.  That shifts everything in the array down by `shifts`.  The function is **circular** so the value in element 0 is moved to be the final value in the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoregressive_iterative_forecast(model, exog, h):\n",
    "    '''\n",
    "    h-step forecast for an autoregressive \n",
    "    model using the iterative prediction method.\n",
    "    \n",
    "    Conduct h one-step forecasts gradually\n",
    "    replacing ground truth autoregressive X \n",
    "    values with predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    ------\n",
    "    model: forecast object\n",
    "        model that has a .predict(h) interface\n",
    "        \n",
    "    exog: array-like\n",
    "        initial vector of lagged values (X)\n",
    "    \n",
    "    h: int\n",
    "        forecast horizon. assumed to be > 0\n",
    "    \n",
    "    Returns:\n",
    "    ------\n",
    "    numpy.ndarray\n",
    "        y_predictions\n",
    "    '''\n",
    "    y_preds = []\n",
    "    current_X = exog\n",
    "    for i in range(h):\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(current_X)[0]\n",
    "        y_preds.append(y_pred)\n",
    "\n",
    "        current_X = torch.roll(current_X, shifts=-1)\n",
    "        current_X[-1] = y_pred\n",
    "\n",
    "    return np.array(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 5\n",
    "y_preds = autoregressive_iterative_forecast(model, X_test[0], h=H)\n",
    "print(f'Iterative forecast: {y_preds}')\n",
    "print(f'Ground truth y: {y_test[:H].numpy().T}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding some noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this a bit more interesting we will add some normally distributed noise to the synthetic time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the random seed so that we all get the same results\n",
    "np.random.seed(12)\n",
    "t = np.arange(200)\n",
    "ts_data = np.cos(0.2 * t)\n",
    "noise = np.random.normal(loc=0.0, scale=0.2, size=200)\n",
    "ts_data = ts_data + noise\n",
    "plt.plot(ts_data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 12\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "#preprocess time series into a supervised learning problem\n",
    "X_data, y_data = sliding_window(ts_data, window_size=WINDOW_SIZE)\n",
    "\n",
    "#train test split\n",
    "train_size = int(len(y_data) * (2/3))\n",
    "X_train, X_test, y_train, y_test = ts_train_test_split(X_data, \n",
    "                                                y_data,\n",
    "                                                train_size=train_size,\n",
    "                                                as_tensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, optimizer, criterion, n_epochs, \n",
    "        X_train, y_train, X_test=None, y_test=None, \n",
    "        batch_size=32, verbose=0):\n",
    "    '''\n",
    "    train the pytorch model \n",
    "    \n",
    "    Parameters:\n",
    "    ------\n",
    "    model: torch.nn.module\n",
    "        PyTorch Neural Network Model implements .forward()\n",
    "    \n",
    "    optimizer: torch.optim.Optimizer\n",
    "        PyTorch optimization engine e.g. Adam\n",
    "        \n",
    "    criterion: torch.nn.criterion\n",
    "        PyTorch criterion e.g. MSELoss \n",
    "        \n",
    "    n_epochs: int\n",
    "        Number of epochs to train\n",
    "        \n",
    "    X_train: Tensor\n",
    "        x training matrix 2D\n",
    "        \n",
    "    y_train: Tensor\n",
    "        y training vector\n",
    "        \n",
    "    X_test: Tensor, optional (default=None)\n",
    "        x test matrix 2D\n",
    "        \n",
    "    y_test: Tensor, optional (default=None)\n",
    "        y test vector\n",
    "        \n",
    "    batch_size: int, optional (default=32)\n",
    "        Size of the mini batches used in training\n",
    "        \n",
    "    verbose: int, optional (default=0)\n",
    "        0 == no output during training\n",
    "        1 == output loss every 10 epochs \n",
    "        (includes validation loss if X_test, y_test included)\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "        dict\n",
    "        training and validation loss history. keys are\n",
    "        'loss' and 'val_loss'\n",
    "    '''\n",
    "    PRINT_STEPS = 10\n",
    "\n",
    "    # Set up lists for loss\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    history = {'loss':[],\n",
    "               'val_loss':[]}\n",
    "\n",
    "    #create the mini-batches\n",
    "    train_loader = get_data_loader(X_train, \n",
    "                               y_train, \n",
    "                               batch_size=batch_size)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Loop through required number of epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        # Train model (using batches): Switch to training mode\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            \n",
    "            y_pred = model.forward(batch[0])\n",
    "            loss = criterion(y_pred, batch[1].reshape(y_pred.shape[0], -1))\n",
    "\n",
    "            # Zero gradients, perform a backward pass,  \n",
    "            # and update the weights. \n",
    "            optimizer.zero_grad() \n",
    "            loss.backward() \n",
    "            optimizer.step() \n",
    "\n",
    "        # Get results for complete training set: Switch to evaluation mode\n",
    "        model.eval()\n",
    "        y_pred_train = model.forward(X_train)\n",
    "        history['loss'].append(criterion(y_pred_train, \n",
    "                                         y_train.reshape(y_pred_train.shape[0], -1)).detach())\n",
    "\n",
    "        if not X_test is None:\n",
    "            # Get results for test set\n",
    "            y_pred_test = model.forward(X_test)\n",
    "            history['val_loss'].append(criterion(y_pred_test, \n",
    "                                                 y_test.reshape(y_pred_test.shape[0], -1)).detach())\n",
    "\n",
    "        # Print loss & accuracy every 10 epochs (print last iteem of results lists)\n",
    "        if verbose == 1:\n",
    "            if (epoch+1) % PRINT_STEPS == 0:\n",
    "                print(f'Epoch {epoch+1}. ', end='')\n",
    "                print(f\"Train accuracy {history['loss'][-1]: 0.3f}. \", end='')\n",
    "                print(f\"Test accuracy {history['val_loss'][-1]: 0.3f}.\")\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    if verbose == 1:\n",
    "        print(f'Training time {duration:.2f}s')\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "\n",
    "# Create model\n",
    "model = LinearModel(WINDOW_SIZE)\n",
    "# Set loss \n",
    "criterion = nn.MSELoss()\n",
    "# Set optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = fit(model, optimizer, criterion, n_epochs=100, \n",
    "              X_train=X_train, y_train=y_train, \n",
    "              X_test=X_test, y_test=y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['loss'], label='loss')\n",
    "plt.plot(history['val_loss'], label='val_loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ts_data[WINDOW_SIZE+1:], label='ground truth')\n",
    "plt.plot(model(X_train).detach().numpy(), label='NN fitted')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make iterative predictions\n",
    "H = len(y_test)\n",
    "y_preds_iter = autoregressive_iterative_forecast(model, \n",
    "                                                 X_test[0], \n",
    "                                                 h=H)\n",
    "\n",
    "#plot\n",
    "plt.plot(y_preds_iter, label='iterative forecast method')\n",
    "plt.plot(y_test, label='ground truth')\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.5 The direct h-step forecasting method.\n",
    "\n",
    "In the direct method to forecast h-steps ahead we have **$h$ forecasting models**.  Each model provides a single point forecast from a step ahead.  In the example here, y_test is of length 57 periods.  The direct method requires 57 NNs to make its prediction!\n",
    "\n",
    "\n",
    "Recall the `sliding_window` function.  We ignored an optional parameter `horizon` in the iterative example.  By default `horizon=1` i.e. the function returns target values that are only a single period ahead.  We can vary the step size by increasing the value of horizon.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training multiple models**\n",
    "\n",
    "1. Create a for loop and set it to iterate 57 times. \n",
    "2. In each loop call `sliding_window` setting `horizon` to the iteration number + 1\n",
    "3. Create a new instance of the the model\n",
    "4. Train the model and save in a list.\n",
    "5. Save the model to .h5 file. (recommended so you can reload without retraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_direct_models(data, n_epochs, horizon, window_size, \n",
    "                        train_size, save=True):\n",
    "\n",
    "    models = []\n",
    "\n",
    "    print('Training model =>', end=' ')\n",
    "    for h in range(horizon):\n",
    "        print(f'{h+1}', end=', ')\n",
    "\n",
    "        # preprocess time series into a supervised learning problem\n",
    "        X, y = sliding_window(data, window_size=window_size,\n",
    "                              horizon=h+1)\n",
    "        \n",
    "        # train test split\n",
    "        X_train, X_test, y_train, y_test = ts_train_test_split(X, y,\n",
    "                                                        train_size=train_size,\n",
    "                                                        as_tensors=True)\n",
    "        # Create model\n",
    "        model_h = LinearModel(window_size)\n",
    "        # Set loss \n",
    "        criterion = nn.MSELoss()\n",
    "        # Set optimizer\n",
    "        optimizer = torch.optim.Adam(model_h.parameters(), lr=0.01)\n",
    "\n",
    "        # fit model silently (verbose=0)\n",
    "        history = fit(model_h, optimizer, criterion, n_epochs=100, \n",
    "                      X_train=X_train, \n",
    "                      y_train=y_train,\n",
    "                      verbose=0)\n",
    "\n",
    "        if save:\n",
    "            path = f'./output/direct_model_h{h+1}.pt'\n",
    "            torch.save(model_h.state_dict(), path)\n",
    "\n",
    "        models.append(model_h)\n",
    "\n",
    "    print('done')\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_linear_direct_models(horizon, ws):\n",
    "    models = []\n",
    "    print('Loading direct model horizon =>', end=' ')\n",
    "    for h in range(horizon):\n",
    "        print(f'{h+1},', end=' ')\n",
    "        model_h = LinearModel(ws)\n",
    "        path = f'./output/direct_model_h{h+1}.pt'\n",
    "        model_h.load_state_dict(torch.load(path))\n",
    "        model_h.eval()\n",
    "        models.append(model_h)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set pytorch random seed\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "N_EPOCHS = 100\n",
    "HORIZON = len(y_test)\n",
    "WINDOW_SIZE = 12\n",
    "TRAIN_SIZE = 130\n",
    "LOAD_FROM_FILE = True\n",
    "\n",
    "if LOAD_FROM_FILE:\n",
    "    direct_models = load_linear_direct_models(HORIZON, \n",
    "                                              WINDOW_SIZE)\n",
    "else:\n",
    "    direct_models = train_direct_models(ts_data, \n",
    "                                        n_epochs=N_EPOCHS,\n",
    "                                        horizon=HORIZON, \n",
    "                                        window_size=WINDOW_SIZE, \n",
    "                                        train_size=TRAIN_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the `direct_forecast` function.  This is just a for loop to pass the input X data each model.  Remember that the input to each model is **same** i.e. exog which in our case will be `X_test[0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_forecast(models, exog):\n",
    "    '''\n",
    "    h-step forecast for an autoregressive \n",
    "    model using the direct prediction method.\n",
    "    \n",
    "    Each model contained in @models has been trained\n",
    "    to predict a unique number of steps ahead. \n",
    "    Each model forecasts and the results are \n",
    "    combined in an ordered array and returned.\n",
    "    \n",
    "    Parameters:\n",
    "    ------\n",
    "    models: list\n",
    "        direct models each has has a .predict(exog) \n",
    "        interface\n",
    "        \n",
    "    exog: Tensor\n",
    "        initial vector of lagged values (X)\n",
    "        \n",
    "    Returns:\n",
    "    ------\n",
    "    numpy.ndarray\n",
    "        y_predictions\n",
    "    '''\n",
    "    preds = []\n",
    "    for model_h in models:\n",
    "        with torch.no_grad():\n",
    "            pred_h = model_h(exog)[0]\n",
    "        preds.append(pred_h)\n",
    "    \n",
    "    return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the direct forecast\n",
    "y_preds_direct = direct_forecast(direct_models, X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the direct forecast against the test data\n",
    "plt.plot(y_preds_direct, label='direct forecast method')\n",
    "plt.plot(y_test, label='ground truth')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the iterative method the direct method looks a close match to the ground truth test set!  Let's plot all three datasets on the same chart and then take a look at the **RMSE** of each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot iterative and direct\n",
    "plt.plot(y_preds_direct, label='direct forecast method')\n",
    "plt.plot(y_preds_iter, label='iterative forecast method')\n",
    "plt.plot(y_test, label='ground truth', linestyle='-.')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tools.eval_measures import rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(y_test, y_preds_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(y_test, y_preds_direct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular example (and single holdout set) the direct method out performed the iterative method. You should not assume this is always the case!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Forecasting a vector of y\n",
    "\n",
    "In the **iterative** and **direct** methods we always forecast a *scalar* value.  An modification is to adapt a feedforward neural network to predict a **vector of y values**.  Using this architecture we would train our model on sliding windows of $X$ and $y$.  Where y is a vector of length $h$ and $X$ is a vector of length $ws$ (window size)\n",
    "\n",
    "### 2.6.1 Exercise: preprocessing the time series into vectors of y\n",
    "Task: modify the function `sliding_window` (provided below) so that it returns a vectors of y.\n",
    "\n",
    "Hints:\n",
    "* Assume you are standing at time $t$. With a forecasting horizon of $h$, y would be $[y_{t+1}, y_{t+2}, ... , y_{t+h}]$.\n",
    "* Array slicing might prove useful:\n",
    "\n",
    "```python\n",
    "train = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "print(train[2:6])\n",
    "```\n",
    "```\n",
    ">> [3 4 5 6]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(train, window_size=2, horizon=2):\n",
    "    '''\n",
    "    sliding window.\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    train: array-like\n",
    "        training data for time series method\n",
    "    \n",
    "    window_size: int, optional (default=2)\n",
    "        lookback - how much lagged data to include.\n",
    "        \n",
    "    horizon: int, optional (default=2)\n",
    "        number of observations ahead to predict\n",
    "            \n",
    "    Returns:\n",
    "        array-like, array-like\n",
    "    \n",
    "        preprocessed X, preprocessed Y\n",
    "    '''\n",
    "    tabular_X = []\n",
    "    tabular_y = []\n",
    "    \n",
    "    for i in range(0, len(train) - window_size - horizon):\n",
    "        X_train = train[i:window_size+i]\n",
    "        #we use list slicing to return a vector of training for y_train\n",
    "        y_train = train[i+window_size:i+window_size+horizon]\n",
    "        tabular_X.append(X_train)\n",
    "        tabular_y.append(y_train)\n",
    "       \n",
    "    return np.asarray(tabular_X), np.array(tabular_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After** you have modified `sliding_window` run the code below to preprocess the time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6.2 Build a model that predicts vectors in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model on PyTorch nn.Module class\n",
    "class VectorModel(nn.Module):\n",
    "    def __init__(self, in_features, n_neurons, n_outputs):\n",
    "        # Inherit parent (nn.module) methods using super init\n",
    "        super(VectorModel, self).__init__()\n",
    "        \n",
    "        # This time we have two Linear layers\n",
    "        \n",
    "        # The first layer maps input features to a second layer\n",
    "        # with user specified number of neurons.\n",
    "        self.layer1 = nn.Linear(in_features=in_features, \n",
    "                                out_features=n_neurons, \n",
    "                                bias=True)\n",
    "        \n",
    "        # The output layer has a user specified n_outputs\n",
    "        # This is equal to the horizon you are predicting.\n",
    "        self.out = nn.Linear(in_features=n_neurons,\n",
    "                             out_features=n_outputs,\n",
    "                             bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass data through net. \n",
    "        x = F.relu(self.layer1(x))\n",
    "        y_pred = self.out(x)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.3 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set PyTorch random seed\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "WINDOW_SIZE = 12\n",
    "HORIZON = 12\n",
    "N_EPOCHS = 100\n",
    "TRAIN_SIZE = 130\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# convert time series to supervised learning format\n",
    "X, y = sliding_window(ts_data, \n",
    "                      window_size=WINDOW_SIZE,\n",
    "                      horizon=HORIZON)\n",
    "\n",
    "#train-test split\n",
    "X_train, X_test, y_train, y_test = ts_train_test_split(X, y,\n",
    "                                                       train_size=TRAIN_SIZE,\n",
    "                                                       as_tensors=True)\n",
    "\n",
    "# Create an instance of VectorModel\n",
    "model_v = VectorModel(WINDOW_SIZE, n_neurons=10, n_outputs=HORIZON)\n",
    "# Set loss function\n",
    "criterion = nn.MSELoss()\n",
    "# Set optimizer to Adam\n",
    "optimizer = torch.optim.Adam(model_v.parameters(), lr=0.01)\n",
    "\n",
    "# train the model\n",
    "history = fit(model_v, optimizer, criterion, N_EPOCHS, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['loss'], label='loss')\n",
    "plt.plot(history['val_loss'], label='val_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.4 Predict a single vector ahead.\n",
    "\n",
    "Predicting a single vector ahead is actually making a h-step forecast.  This is done in exactly the same way as the other models using `.predict(X)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_preds = model_v(X_test[0])\n",
    "y_preds.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the prediction\n",
    "plt.plot(y_test[0], label='test')\n",
    "plt.plot(y_preds, label='forecast')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.5 Exercise predicting multiple vectors ahead (> h-steps)\n",
    "\n",
    "It is important to remember that with the vector output you predict in multiples of $h$.  So if you make two predictions you have predictions for 2h.  But in general this works in the same way as the iterative method.  Each time you forecast you replace $h$ values in the X vector with the predictions.\n",
    "\n",
    "**Task**: \n",
    "* Modify `autoregressive_iterative_forecast` (provided below) so that it works with the new model.  After you are done rename the function `vector_iterative_forecast`\n",
    "* Predict 4 vector lengths ahead and plot the result against the test set.\n",
    "\n",
    "Hints:\n",
    "\n",
    "* For simplicity, you could make the parameter `h` the number of vectors ahead to predict.\n",
    "* Each call of `model(X)` returns a vector.  At the end of the iterative forecast you will have a list of vectors.  Call `np.concatenate(list)` to transform this into a single list.\n",
    "* In the notebook the X and vectors are both of size 12 (`WINDOW_SIZE == 12` and `len(y_train[0]) == 12`). This means you could simplify your code for the example.  Alternatively it could work with different sized X and y vectors. \n",
    "* Remember that `y_test` contains sliding windows of size 12.  So if you predict 2 vectors ahead then you will need to plot `y_test[0]` and `y_test[12]`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_iterative_forecast(model, exog, h):\n",
    "    '''\n",
    "    h-step forecast for an autoregressive \n",
    "    model using the iterative prediction method.\n",
    "    \n",
    "    Conduct h one-step forecasts gradually\n",
    "    replacing ground truth autoregressive X \n",
    "    values with predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    ------\n",
    "    model: forecast object\n",
    "        model that has a .predict(h) interface\n",
    "        \n",
    "    exog: array-like\n",
    "        initial vector of lagged values (X)\n",
    "    \n",
    "    h: int\n",
    "        forecast horizon. assumed to be > 0\n",
    "    \n",
    "    Returns:\n",
    "    ------\n",
    "    numpy.ndarray\n",
    "        y_predictions\n",
    "    '''\n",
    "    y_preds = []\n",
    "    current_X = exog\n",
    "    for i in range(h):\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(current_X)\n",
    "            \n",
    "        y_preds.append(y_pred.numpy())\n",
    "        \n",
    "        #current_X = np.roll(current_X, shift=-h)\n",
    "        #current_X[-h] = y_pred.copy()\n",
    "        #in pytorch we use clone() method \n",
    "        current_X = y_pred.clone()\n",
    "\n",
    "    return np.concatenate(np.array(y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 4\n",
    "y_preds = vector_iterative_forecast(model_v, X_test[0], H)\n",
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_to_plot = []\n",
    "for i in range(H):\n",
    "    y_test_to_plot.append(y_test[WINDOW_SIZE*i].numpy())\n",
    "\n",
    "plt.plot(np.concatenate(y_test_to_plot), label='test')\n",
    "plt.plot(y_preds, label='forecast')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
