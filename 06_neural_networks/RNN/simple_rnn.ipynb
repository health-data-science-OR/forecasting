{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Recurrent Neural Networks\n",
    "\n",
    "In this notebook we will use a Simple Recurrent Neural Network (RNN) to predict a health care time series.  Although the method is called 'simple' RNN's are more complicated than the Feedforward Neural Networks we have already used.  \n",
    "\n",
    ">Note that increased complexity does not always come with improved performance!\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "1. The data preprocessing requirements for a RNN are almost identical to feedforward networks.\n",
    "    \n",
    "2. Feedforward networks expected X_train data in two dimensions (n_time_steps, window_size). In contrast, RNNs expect X_train data in three dimensions (n_time_steps, window_size, n_features).\n",
    "\n",
    "2. In practice, forecasting **h-steps** ahead is best achieved by predicting a target vector.\n",
    "</div>\n",
    "\n",
    "---\n",
    "**LEARNING OBJECTIVES**\n",
    "\n",
    "* Understand how to use Keras to build a SimpleRNN\n",
    "* Learn the important of the shape of input data for an RNN.\n",
    "* Generate h-step forecasts using an iterative approach\n",
    "* Use a ensemble of SimpleRNNs to produce a better forecast.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Python dependencies\n",
    "\n",
    "It is recommended that you use the forecasting course conda environment provided for this work. We are going to implement neural networks using `tensorflow` and '`keras`. You should be using at least `tensorflow` version `2.1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# tensorflow imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import (Input, Dense, Flatten, SimpleRNN, \n",
    "                                     LSTM, GRU, GlobalMaxPool1D, Conv1D)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# turn off tensorflow warnings when using predict in a loop.\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset: Emergency admissions in England\n",
    "\n",
    "We will again use the monthly emergency admissions in England dataset.  \n",
    "\n",
    "### 2.1 Load and preprocess data\n",
    "\n",
    "**Summary of the code below**:\n",
    "* Reads the ed_admits dataset.\n",
    "* Parses the date column\n",
    "* Calender adjusts the data\n",
    "* Standardises the data between -1 and 1 using MinMaxScaler\n",
    "* Converts the time series to a supervised learning problem with window_size 12 and target vector length 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/health-data-science-OR/data' \\\n",
    "        + '/master/em_admits_ts.csv'\n",
    "em_admits = pd.read_csv(url)\n",
    "date_str = em_admits['month_year'].str[:3] + ' 20' + \\\n",
    "    em_admits['month_year'].str[-2:]\n",
    "date_str.name = 'date'\n",
    "em_admits = em_admits.set_index(pd.to_datetime(date_str))\n",
    "em_admits.index.freq = 'MS'\n",
    "em_admits = em_admits.drop(columns=['month_year'])\n",
    "admit_rate = em_admits['em_admits'] / em_admits.index.days_in_month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(train, window_size=2, horizon=2):\n",
    "    '''\n",
    "    sliding window.\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    train: array-like\n",
    "        training data for time series method\n",
    "    \n",
    "    window_size: int, optional (default=2)\n",
    "        lookback - how much lagged data to include.\n",
    "        \n",
    "    horizon: int, optional (default=2)\n",
    "        number of observations ahead to predict\n",
    "            \n",
    "    Returns:\n",
    "        array-like, array-like\n",
    "    \n",
    "        preprocessed X, preprocessed Y\n",
    "    '''\n",
    "    tabular_X = []\n",
    "    tabular_y = []\n",
    "    \n",
    "    for i in range(0, len(train) - window_size - horizon):\n",
    "        X_train = train[i:window_size+i]\n",
    "        # we use list slicing to return a vector of training for y_train\n",
    "        y_train = train[i+window_size:i+window_size+horizon]\n",
    "        tabular_X.append(X_train)\n",
    "        tabular_y.append(y_train)\n",
    "    \n",
    "    return np.asarray(tabular_X), np.array(tabular_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It is critical that we get the X training data into the correct shape.  The function `sliding_window` generates a 2D array (n_time_steps, window_size).  Off the array call the `.reshape(-1, WINDOW_SIZE, 1)` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess time series training and test sets\n",
    "WINDOW_SIZE = 12\n",
    "HORIZON = 12\n",
    "TRAIN_LENGTH = 56\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "#I am scaling on admit_rate because this will include the first 12 lags \n",
    "#not in y_train\n",
    "scaler.fit(admit_rate.iloc[:-12].to_numpy().reshape(-1, 1))\n",
    "admit_rate_adj = scaler.transform(admit_rate.to_numpy().reshape(-1, 1))\n",
    "\n",
    "#convert to supervised learning problem\n",
    "X_train, y_train = sliding_window(admit_rate_adj, \n",
    "                                  window_size=WINDOW_SIZE,\n",
    "                                  horizon=HORIZON)\n",
    "\n",
    "#This is the key part we reshape to a 3D representation \n",
    "#(n_time_steps, window_size, n_features)\n",
    "X_train = X_train.reshape(-1, WINDOW_SIZE, 1)\n",
    "\n",
    "#train-test split\n",
    "X_train, X_test = X_train[:TRAIN_LENGTH], X_train[TRAIN_LENGTH:]\n",
    "y_train, y_test = y_train[:TRAIN_LENGTH], y_train[TRAIN_LENGTH:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Check `X_train` and `X_test` shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling a SimpleRNN in Keras\n",
    "\n",
    "In Keras, we built a multi-layered feedforward neural network for time series like so:\n",
    "```python\n",
    "\n",
    "HORIZON = 12\n",
    "WINDOW_SIZE = 12\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(WINDOW_SIZE, 1)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(HORIZON))\n",
    "model.compile(optimizer=Adam(lr=0.01), loss='mse')\n",
    "```\n",
    "Keras provides an RNN layer\n",
    "\n",
    "```Python\n",
    "tensorflow.keras.layers.SimpleRNN\n",
    "```\n",
    "\n",
    "To build the RNN we just replace the first `Dense` layer with the `SimpleRNN` class from Keras:\n",
    "\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(WINDOW_SIZE, 1)))\n",
    "model.add(SimpleRNN(units=100, activation='tanh'))\n",
    "model.add(Dense(HORIZON))\n",
    "...\n",
    "```\n",
    "\n",
    ">Note that the default activation function for `SimpleRNN` is the hyperbolic tangent (tanh).  In practice you may want to compare the result to an RNN using a Linear Rectifier (ReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Create a SimpleRNN model.\n",
    "\n",
    "* Create a function called `get_simple_rnn()`  \n",
    "* The function will build and compile a SimpleRNN model.\n",
    "* The function should accept the parameters\n",
    " * window_size: int, the number lags of the time series included in each training observation\n",
    " * n_units: int, the number of units to include in the RNN. An optional paramter (default=100)\n",
    " * activation: str, the SimpleRNN activation function. An optional parameter (efault is 'tanh')\n",
    " * The function must return a compiled Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_rnn(window_size, n_units=10, activation='tanh'):\n",
    "    '''\n",
    "    Build and compile a simple RNN in Keras\n",
    "    \n",
    "    Parameters:\n",
    "    -------\n",
    "    window_size: int, \n",
    "        the number lags of the time series included in each training observation\n",
    "    \n",
    "    n_units: int, optional (default=100)\n",
    "        the number of units to include in the RNN.\n",
    "    \n",
    "    activation: str, optional (default='tanh')\n",
    "        the SimpleRNN activation function.\n",
    "    \n",
    "    Returns:\n",
    "    ------\n",
    "    A compiled Keras model.\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(window_size, 1)))\n",
    "    model.add(SimpleRNN(n_units, activation=activation))\n",
    "    model.add(Dense(window_size))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the SimpleRNN\n",
    "\n",
    "Training is the same as feedforward neural networks.  The code below illustrates how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set tensorflow random seed for repeatability\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "N_EPOCHS = 100\n",
    "\n",
    "#It might be worth experimenting with early stopping by varying @patience\n",
    "es = EarlyStopping(monitor='val_loss', patience=20,\n",
    "                  restore_best_weights=True)\n",
    "\n",
    "#this will only work if you have coded get_simple_rnn correctly!\n",
    "model = get_simple_rnn(12, n_units=100, activation='tanh')\n",
    "\n",
    "#train the model silently (verbose=0)\n",
    "history = model.fit(x=X_train, \n",
    "                    y=y_train, \n",
    "                    epochs=N_EPOCHS,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    verbose=0,\n",
    "                    callbacks=[es], \n",
    "                    batch_size=16)\n",
    "\n",
    "#plot the training and validation loss\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasting with the RNN\n",
    "\n",
    "> There are a couple of points to remember when forecasting.  The first is that an RNN expects a 3D input array.  The second is that we we trained our SimpleRNN to predict a vector of length 12.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = model.predict(X_test[0].reshape(1, WINDOW_SIZE, -1))[0]\n",
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#back transform the predictions\n",
    "scaler.inverse_transform(y_preds.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#back transform the matching y_test vector\n",
    "scaler.inverse_transform(y_test[0].reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the two predictions on the same chart\n",
    "plt.plot(scaler.inverse_transform(y_preds.reshape(-1, 1)), \n",
    "         label='forecast')\n",
    "plt.plot(scaler.inverse_transform(y_test[0].reshape(-1, 1)), \n",
    "         label='ground_truth')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a h-step forecast using an RNN that predicts a vector.\n",
    "\n",
    "We again use the iterative method for forecasting, but need to remember that we are predicting vectors on each iteration.  If our vector is of length 12 and we we want to predict 24 months ahead then we make 2 iterative predictions.  \n",
    "\n",
    ">This is the same method we used with feedforward networks.  The function `vector_iterative_forecast` is very similar, but the key difference is that an RNN expects 3D data where ANN expected 2D. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_iterative_forecast(model, exog, h):\n",
    "    '''\n",
    "    h-step forecast for an autoregressive \n",
    "    model using the iterative prediction method.\n",
    "    \n",
    "    Conduct h one-step forecasts gradually\n",
    "    replacing ground truth autoregressive X \n",
    "    values with predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    ------\n",
    "    model: forecast object\n",
    "        model that has a .predict(h) interface\n",
    "        \n",
    "    exog: array-like\n",
    "        initial vector of lagged values (X)\n",
    "    \n",
    "    h: int\n",
    "        forecast horizon. assumed to be > 0\n",
    "    \n",
    "    Returns:\n",
    "    ------\n",
    "    numpy.ndarray\n",
    "        y_predictions\n",
    "    '''\n",
    "    y_preds = []\n",
    "    current_X = exog\n",
    "    for i in range(h):\n",
    "        \n",
    "        #this is the key change we are resizing to 3D\n",
    "        y_pred = model.predict(current_X.reshape(1, WINDOW_SIZE, -1))[0]\n",
    "        y_preds.append(y_pred)\n",
    "        \n",
    "        #current_X = np.roll(current_X, shift=-h)\n",
    "        #current_X[-h] = y_pred.copy()\n",
    "        current_X = y_pred.copy()\n",
    "\n",
    "    return np.concatenate(np.array(y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to help plotting\n",
    "\n",
    "def plot_nn_prediction_results(model, X_train, y_train, y_test, y_preds):  \n",
    "    \n",
    "    #create series\n",
    "    fitted_values = scaler.inverse_transform(model.predict(X_train))\n",
    "    ground_truth = scaler.inverse_transform(y_train)\n",
    "    ground_truth_val = scaler.inverse_transform(y_test)\n",
    "\n",
    "    padding = np.full(len(fitted_values), np.NAN)\n",
    "\n",
    "    validation = np.concatenate([padding.reshape(-1, 1), ground_truth_val])\n",
    "    forecast = np.concatenate([padding.reshape(-1, 1), y_preds])\n",
    "\n",
    "    plt.plot(ground_truth, label='ground truth')\n",
    "    plt.plot(validation, label='test')\n",
    "    plt.plot(fitted_values, label='in-sample', linestyle='-.')\n",
    "    plt.plot(forecast, label='out-of-sample', linestyle='-.')\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict next 24 months and plot (2 vector lengths)\n",
    "H = 24\n",
    "VECTORS_AHEAD = H // WINDOW_SIZE\n",
    "\n",
    "y_preds = vector_iterative_forecast(model, X_test[0], h=VECTORS_AHEAD)\n",
    "y_preds = scaler.inverse_transform(y_preds.reshape(-1, 1))\n",
    "\n",
    "#plot_nn_prediction_results(model, X_train, y_train, y_test[0], y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_to_plot = []\n",
    "for i in range(VECTORS_AHEAD):\n",
    "    y_test_to_plot.append(y_test[WINDOW_SIZE*i])\n",
    "y_test_to_plot = np.concatenate(y_test_to_plot)\n",
    "\n",
    "y_test_to_plot = scaler.inverse_transform(y_test_to_plot.reshape(-1, 1))   \n",
    "plt.plot(y_test_to_plot, label='test')\n",
    "plt.plot(y_preds, label='forecast')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_rnn = rmse(scaler.inverse_transform(y_test[0].reshape(-1, 1)), y_preds.T[:H])[0]\n",
    "print(f'rmse RNN: {rmse_rnn:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with the SimpleRNN\n",
    "\n",
    "**Exercise**: \n",
    "\n",
    "Select a number of training epochs (e.g. 100-200) and try the following experiments to explore how the SimpleRNN behaves (note in all of these window_size is 12)\n",
    "* n_units=100, activation='tanh'\n",
    "* n_units=150, activation='tanh'\n",
    "* n_units=100, activation='relu'\n",
    "* n_units=150, activation='relu'\n",
    "\n",
    "**Hints:**\n",
    "* Remember to plot the training and validation loss\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise create an ensemble of SimpleRNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training will take a few minutes.  Put the kettle on.\n",
    "\n",
    "#set tensorflow random seed for repeatability\n",
    "tf.random.set_seed(1066)\n",
    "\n",
    "N_MODELS = 20\n",
    "N_EPOCHS = 100\n",
    "N_UNITS = 100\n",
    "H = 12\n",
    "VECTORS_AHEAD = H // WINDOW_SIZE\n",
    "\n",
    "es = EarlyStopping(monitor='loss', patience=10)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "models = []\n",
    "for n in range(N_MODELS):\n",
    "    #SimpleRNN model\n",
    "    model_n = model = get_simple_rnn(WINDOW_SIZE, \n",
    "                                     n_units=N_UNITS, \n",
    "                                     activation='tanh')\n",
    "\n",
    "    #fit model silently\n",
    "    history = model_n.fit(x=X_train, \n",
    "                          y=y_train, \n",
    "                          epochs=N_EPOCHS,\n",
    "                          verbose=0, \n",
    "                          callbacks=[es], \n",
    "                          batch_size=BATCH_SIZE)\n",
    "\n",
    "    #this will overwrite pre-trained models.\n",
    "    #model_n.save(f'output/mlp_ensemble_{n}.h5')\n",
    "    models.append(model_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code will take a few seconds to execute\n",
    "H = 1\n",
    "e_preds = []\n",
    "for model in models:\n",
    "    y_preds = vector_iterative_forecast(model, X_test[0], h=H)\n",
    "    e_preds.append(y_preds)\n",
    "    \n",
    "e_preds = np.array(e_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_preds = np.asarray(e_preds)\n",
    "e_preds_tran = scaler.inverse_transform(e_preds).T\n",
    "y_preds_mdn = np.percentile(e_preds_tran.T, 50, axis=0)\n",
    "y_preds_2_5 = np.percentile(e_preds_tran.T, 2.5, axis=0)\n",
    "y_preds_97_5 = np.percentile(e_preds_tran.T, 97.5, axis=0)\n",
    "y_preds_mdn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the individual forecasts and the median\n",
    "\n",
    "fig,ax = plt.subplots(1, 2, sharey=True, figsize=(12, 4))\n",
    "ax[0].plot(e_preds_tran)\n",
    "ax[0].plot(scaler.inverse_transform(y_test[0]), label='test', linestyle='--', \n",
    "         color='red')\n",
    "ax[0].plot(y_preds_mdn, label='median', linestyle='-', color='black')\n",
    "ax[0].legend()\n",
    "ax[0].set_title(f'Point forecasts: {N_MODELS} models')\n",
    "\n",
    "ax[1].plot(scaler.inverse_transform(y_test[0]), label='test', linestyle='--', \n",
    "         color='red')\n",
    "ax[1].plot(y_preds_mdn, label='median', linestyle='-', color='black')\n",
    "ax[1].plot(y_preds_2_5, label='0.025 percentile', linestyle='-.', color='black')\n",
    "ax[1].plot(y_preds_97_5, label='0.975 percentile', linestyle='--', color='black')\n",
    "\n",
    "ax[1].set_title(f'Middle 95% of point forecasts ')\n",
    "ax[1].legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_rnn_mdn = rmse(scaler.inverse_transform(y_test[0].reshape(-1, 1)), y_preds_mdn)[0]\n",
    "\n",
    "print(f'rmse RNN: {rmse_rnn_mdn:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
